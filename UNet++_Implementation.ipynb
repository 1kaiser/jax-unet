{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1kaiser/jax-unet/blob/master/UNet%2B%2B_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sure, here is an example of how to use the Nested UNet model to segment objects in an image:"
      ],
      "metadata": {
        "id": "0aDT2cHDko0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class conv_block_nested(nn.Module):\n",
        "\n",
        "    def __init__(self, in_ch, mid_ch, out_ch):\n",
        "        super(conv_block_nested, self).__init__()\n",
        "        self.activation = nn.ReLU(inplace=True)\n",
        "        self.conv1 = nn.Conv2d(in_ch, mid_ch, kernel_size=3, padding=1, bias=True)\n",
        "        self.bn1 = nn.BatchNorm2d(mid_ch)\n",
        "        self.conv2 = nn.Conv2d(mid_ch, out_ch, kernel_size=3, padding=1, bias=True)\n",
        "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        output = self.activation(x)\n",
        "\n",
        "        return output\n",
        "\n",
        "class Nested_UNet(nn.Module):\n",
        "\n",
        "    def __init__(self, in_ch=3, out_ch=1):\n",
        "        super(Nested_UNet, self).__init__()\n",
        "\n",
        "        n1 = 64\n",
        "        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 16]\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.Up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "\n",
        "        self.conv0_0 = conv_block_nested(in_ch, filters[0], filters[0])\n",
        "        self.conv1_0 = conv_block_nested(filters[0], filters[1], filters[1])\n",
        "        self.conv2_0 = conv_block_nested(filters[1], filters[2], filters[2])\n",
        "        self.conv3_0 = conv_block_nested(filters[2], filters[3], filters[3])\n",
        "        self.conv4_0 = conv_block_nested(filters[3], filters[4], filters[4])\n",
        "\n",
        "        self.conv0_1 = conv_block_nested(filters[0] + filters[1], filters[0], filters[0])\n",
        "        self.conv1_1 = conv_block_nested(filters[1] + filters[2], filters[1], filters[1])\n",
        "        self.conv2_1 = conv_block_nested(filters[2] + filters[3], filters[2], filters[2])\n",
        "        self.conv3_1 = conv_block_nested(filters[3] + filters[4], filters[3], filters[3])\n",
        "\n",
        "        self.conv0_2 = conv_block_nested(filters[0]*2 + filters[1], filters[0], filters[0])\n",
        "        self.conv1_2 = conv_block_nested(filters[1]*2 + filters[2], filters[1], filters[1])\n",
        "        self.conv2_2 = conv_block_nested(filters[2]*2 + filters[3], filters[2], filters[2])\n",
        "\n",
        "        self.conv0_3 = conv_block_nested(filters[0]*3 + filters[1], filters[0], filters[0])\n",
        "        self.conv1_3 = conv_block_nested(filters[1]*3 + filters[2], filters[1], filters[1])\n",
        "\n",
        "        self.conv0_4 = conv_block_nested(filters[0]*4 + filters[1], filters[0], filters[0])\n",
        "\n",
        "        self.final = nn.Conv2d(filters[0], out_ch, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x0_0 = self.conv0_0(x)\n",
        "        x1_0 = self.conv1_0(self.pool(x0_0))\n",
        "        x0_1 = self.conv0_1(torch.cat([x0_0, self.Up(x1_0)], 1))\n",
        "\n",
        "        x2_0 = self.conv2_0(self.pool(x1_0))\n",
        "        x1_1 = self.conv1_1(torch.cat([x1_0, self.Up(x2_0)], 1))\n",
        "        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, self.Up(x1_1)], 1))\n",
        "\n",
        "        x3_0 = self.conv3_0(self.pool(x2_0))\n",
        "        x2_1 = self.conv2_1(torch.cat([x2_0, self.Up(x3_0)], 1))\n",
        "        x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, self.Up(x2_1)], 1))\n",
        "        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, self.Up(x1_2)], 1))\n",
        "\n",
        "        x4_0 = self.conv4_0(self.pool(x3_0))\n",
        "        x3_1 = self.conv3_1(torch.cat([x3_0, self.Up(x4_0)], 1))\n",
        "        x2_2 = self.conv2_2(torch.cat([x2_0, x2_1, self.Up(x3_1)], 1))\n",
        "        x1_3 = self.conv1_3(torch.cat([x1_0, x1_1, x1_2, self.Up(x2_2)], 1))\n",
        "        x0_4 = self.conv0_4(torch.cat([x0_0, x0_1, x0_2, x0_3, self.Up(x1_3)], 1))\n",
        "\n",
        "        output = self.final(x0_4)\n",
        "        return output"
      ],
      "metadata": {
        "id": "z8O_6Ob_kprv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Load the Nested UNet model\n",
        "model = Nested_UNet(in_ch=3, out_ch=1)\n",
        "\n",
        "# Load the input image\n",
        "image = torch.randn(1, 3, 256, 256)\n",
        "\n",
        "# Set the model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Forward pass the input image through the model\n",
        "output = model(image)\n",
        "\n",
        "# Get the segmentation mask\n",
        "segmentation_mask = output.argmax(1)\n",
        "\n",
        "# Save the segmentation mask\n",
        "torch.save(segmentation_mask, \"segmentation_mask.pt\")"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "id": "cRWzY2Veko0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **serial**\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Nested UNet model\n",
        "model = Nested_UNet(in_ch=3, out_ch=3)\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "dataset = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
        "\n",
        "# Create a data loader\n",
        "data_loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "import cv2\n",
        "# Define a function to plot the predicted images\n",
        "def plot_predictions(images, labels, predictions):\n",
        "    print(len(images))\n",
        "    for i in range(len(images)):\n",
        "      print(type(predictions[i]), type(images[i]))\n",
        "      \n",
        "      cv2_imshow(cv2.cvtColor(np.array(images[i].detach().numpy()).reshape(32, 32, 3), cv2.COLOR_BGR2RGB))\n",
        "      cv2_imshow(cv2.cvtColor(np.array(predictions[i].detach().numpy()).reshape(32, 32, 3), cv2.COLOR_BGR2RGB))\n",
        "\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(10):\n",
        "    for batch in data_loader:\n",
        "        images, labels = batch\n",
        "        predictions = model(images)\n",
        "        print(images.shape, predictions.shape)\n",
        "        loss = nn.CrossEntropyLoss()(predictions, images)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Plot the predicted images\n",
        "        plot_predictions(images, labels, predictions)\n",
        "\n",
        "# Evaluate the model\n",
        "correct = 0\n",
        "total = 0\n",
        "for batch in data_loader:\n",
        "    images, labels = batch\n",
        "    predictions = model(images)\n",
        "    _, predicted = torch.max(predictions.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), \"model.pt\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4iCYoCZHlOhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code will load the Nested UNet model, load the input image, set the model in evaluation mode, forward pass the input image through the model, get the segmentation mask, and save the segmentation mask.\n",
        "\n",
        "The segmentation mask can be used to identify the objects in the image. For example, if the input image is a picture of a cat, the segmentation mask will identify the pixels that belong to the cat. The segmentation mask can be used for a variety of tasks, such as object detection, object tracking, and image editing."
      ],
      "metadata": {
        "id": "nUU3PLhsko0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/1kaiser/Media-Segment-Depth-MLP/releases/download/v0.2/s.zip\n",
        "!unzip '*.zip' -d files\n",
        "!rm -r *.zip"
      ],
      "metadata": {
        "id": "fr2A0eZIDDWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import time\n",
        "#@title **parallel**\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Nested UNet model\n",
        "model = Nested_UNet(in_ch=3, out_ch=3)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    model = model.to(device)\n",
        "    print(\"Model is now running on GPU!\")\n",
        "else:\n",
        "    print(\"GPU is not available!\")\n",
        "\n",
        "\n",
        "\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import ToTensor, Resize\n",
        "import torchvision.transforms as transforms\n",
        "# Get the path to the image folder\n",
        "data_dir = \"/content/files\"\n",
        "transform = transforms.Compose([ToTensor(), Resize((128, 128))])\n",
        "dataset = ImageFolder(data_dir, transform=transform, target_transform=None)\n",
        "print(len(dataset))\n",
        "data_loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "import cv2\n",
        "# Define a function to plot the predicted images\n",
        "def plot_predictions(images, labels, predictions):\n",
        "    print(len(images))\n",
        "    for i in range(len(images)):\n",
        "      print(type(predictions[i]), type(images[i]))\n",
        "      \n",
        "      cv2_imshow(cv2.cvtColor(torch.moveaxis(np.array(images[i].cpu().detach().numpy()), 0, -1), cv2.COLOR_BGR2RGB))\n",
        "      cv2_imshow(cv2.cvtColor(torch.moveaxis(np.array(predictions[i].cpu().detach().numpy()), 0, -1), cv2.COLOR_BGR2RGB))\n",
        "\n",
        "# Train the model\n",
        "start_time = time.time()\n",
        "for epoch in range(1):\n",
        "    for batch in data_loader:\n",
        "        images, labels = batch\n",
        "        images = images.to(device)\n",
        "        # labels = labels.to(device)\n",
        "        predictions = model(images)\n",
        "        loss = nn.CrossEntropyLoss()(predictions, images)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Plot the predicted images\n",
        "        # plot_predictions(images, labels, predictions)\n",
        "        print(time.time() - start_time, \"<<< ⌚ for a batch\")\n",
        "    print(time.time() - start_time, \"<<< ⌛⌛⌛⌛⌛⌛⌛⌛⌛⌛ for a epoch\")\n",
        "# Evaluate the model\n",
        "correct = 0\n",
        "total = 0\n",
        "for batch in data_loader:\n",
        "    images, labels = batch\n",
        "    images = images.to(device)\n",
        "    # labels = labels.to(device)\n",
        "    predictions = model(images)\n",
        "    total += images.size(0)\n",
        "    correct += (predictions == images).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), \"model.pt\")\n",
        "\n",
        "print(\"The time to completion of the model code is:\", time.time() - start_time)\n"
      ],
      "metadata": {
        "id": "2f4MwWfdDYAs",
        "outputId": "16b57db1-9353-48bd-bc0a-dc8a2b0ff5aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model is now running on GPU!\n",
            "2028\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7060868740081787 <<< ⌚ for a batch\n",
            "3.4487032890319824 <<< ⌚ for a batch\n",
            "4.700771331787109 <<< ⌚ for a batch\n",
            "5.9438207149505615 <<< ⌚ for a batch\n",
            "7.194634914398193 <<< ⌚ for a batch\n",
            "8.455477237701416 <<< ⌚ for a batch\n",
            "9.72258973121643 <<< ⌚ for a batch\n",
            "11.015422821044922 <<< ⌚ for a batch\n",
            "12.311810493469238 <<< ⌚ for a batch\n",
            "13.587861061096191 <<< ⌚ for a batch\n",
            "14.943670511245728 <<< ⌚ for a batch\n",
            "16.663172960281372 <<< ⌚ for a batch\n",
            "17.947860717773438 <<< ⌚ for a batch\n",
            "19.229580879211426 <<< ⌚ for a batch\n",
            "20.541556119918823 <<< ⌚ for a batch\n",
            "21.836486339569092 <<< ⌚ for a batch\n",
            "23.125173807144165 <<< ⌚ for a batch\n",
            "24.394511461257935 <<< ⌚ for a batch\n",
            "25.665920972824097 <<< ⌚ for a batch\n",
            "26.940340042114258 <<< ⌚ for a batch\n",
            "28.190704584121704 <<< ⌚ for a batch\n",
            "29.426841020584106 <<< ⌚ for a batch\n",
            "30.67162799835205 <<< ⌚ for a batch\n",
            "31.90028190612793 <<< ⌚ for a batch\n",
            "33.12438631057739 <<< ⌚ for a batch\n",
            "34.35887289047241 <<< ⌚ for a batch\n",
            "35.57278490066528 <<< ⌚ for a batch\n",
            "36.771268367767334 <<< ⌚ for a batch\n",
            "37.98375105857849 <<< ⌚ for a batch\n",
            "39.19115662574768 <<< ⌚ for a batch\n",
            "40.40508580207825 <<< ⌚ for a batch\n",
            "41.580615520477295 <<< ⌚ for a batch\n",
            "42.782463788986206 <<< ⌚ for a batch\n",
            "43.9708456993103 <<< ⌚ for a batch\n",
            "45.13727331161499 <<< ⌚ for a batch\n",
            "46.30967736244202 <<< ⌚ for a batch\n",
            "47.48635292053223 <<< ⌚ for a batch\n",
            "48.65864658355713 <<< ⌚ for a batch\n",
            "49.83638381958008 <<< ⌚ for a batch\n",
            "51.00589108467102 <<< ⌚ for a batch\n",
            "52.18739867210388 <<< ⌚ for a batch\n",
            "53.36469912528992 <<< ⌚ for a batch\n",
            "54.54382514953613 <<< ⌚ for a batch\n",
            "55.70312476158142 <<< ⌚ for a batch\n",
            "56.87031316757202 <<< ⌚ for a batch\n",
            "58.032461166381836 <<< ⌚ for a batch\n",
            "59.186314821243286 <<< ⌚ for a batch\n",
            "60.34053683280945 <<< ⌚ for a batch\n",
            "61.497698068618774 <<< ⌚ for a batch\n",
            "62.662102460861206 <<< ⌚ for a batch\n",
            "63.8224892616272 <<< ⌚ for a batch\n",
            "64.97967600822449 <<< ⌚ for a batch\n",
            "66.12972688674927 <<< ⌚ for a batch\n",
            "67.27257013320923 <<< ⌚ for a batch\n",
            "68.4356951713562 <<< ⌚ for a batch\n",
            "69.59835028648376 <<< ⌚ for a batch\n",
            "70.75908374786377 <<< ⌚ for a batch\n",
            "71.92884635925293 <<< ⌚ for a batch\n",
            "73.08647227287292 <<< ⌚ for a batch\n",
            "74.26300477981567 <<< ⌚ for a batch\n",
            "75.4410285949707 <<< ⌚ for a batch\n",
            "76.61865615844727 <<< ⌚ for a batch\n",
            "77.79204273223877 <<< ⌚ for a batch\n",
            "79.00648736953735 <<< ⌚ for a batch\n",
            "79.00758600234985 <<< ⌛⌛⌛⌛⌛⌛⌛⌛⌛⌛ for a epoch\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c1f880fdb05a>\u001b[0m in \u001b[0;36m<cell line: 66>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;31m# labels = labels.to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-c27f470a3dd9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mx4_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv4_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx3_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mx3_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx3_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx4_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mx2_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx2_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx3_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0mx1_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx1_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mx0_4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv0_4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx0_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 14.75 GiB total capacity; 13.51 GiB already allocated; 8.81 MiB free; 13.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "image_path = '/content/files/annotated_images/out_000000001.png'\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "\n",
        "def predict(image_path):\n",
        "\n",
        "    # Load the model and put it in eval mode\n",
        "    model = torch.load('/content/model.pt')\n",
        "    model.eval()\n",
        "\n",
        "    # Convert the image to a Torch tensor\n",
        "    image = Image.open(image_path)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Resize((128, 128))\n",
        "    ])\n",
        "    image = transform(image)\n",
        "\n",
        "    # Feed the image to the model\n",
        "    predictions = model(image)\n",
        "\n",
        "    # Get the predictions from the model\n",
        "    _, predicted = torch.max(predictions.data, 1)\n",
        "\n",
        "    # Evaluate the predictions\n",
        "    return predicted\n",
        "\n",
        "predict(image_path)"
      ],
      "metadata": {
        "id": "xdS_1keKZes7",
        "outputId": "7a35c0f2-c984-4f5e-908f-fd741ee95b86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-e30b990e335d>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-e30b990e335d>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Load the model and put it in eval mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/model.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Convert the image to a Torch tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'eval'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "#@title **parallel**\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Nested UNet model\n",
        "model = Nested_UNet(in_ch=3, out_ch=3)\n",
        "\n",
        "# Check if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    # Set the device to GPU\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    # Move the model to GPU\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Print a message to the user\n",
        "    print(\"Model is now running on GPU!\")\n",
        "\n",
        "else:\n",
        "\n",
        "    # Print a message to the user\n",
        "    print(\"GPU is not available!\")\n",
        "\n",
        "# Load the dataset\n",
        "dataset = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
        "\n",
        "# Create a data loader\n",
        "data_loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "import cv2\n",
        "# Define a function to plot the predicted images\n",
        "def plot_predictions(images, labels, predictions):\n",
        "    print(len(images))\n",
        "    for i in range(len(images)):\n",
        "      print(type(predictions[i]), type(images[i]))\n",
        "      \n",
        "      cv2_imshow(cv2.cvtColor(np.array(images[i].cpu().detach().numpy()).reshape(32, 32, 3), cv2.COLOR_BGR2RGB))\n",
        "      cv2_imshow(cv2.cvtColor(np.array(predictions[i].cpu().detach().numpy()).reshape(32, 32, 3), cv2.COLOR_BGR2RGB))\n",
        "\n",
        "# Train the model\n",
        "start_time = time.time()\n",
        "for epoch in range(10):\n",
        "    for batch in data_loader:\n",
        "        images, labels = batch\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        predictions = model(images)\n",
        "        loss = nn.CrossEntropyLoss()(predictions, images)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Plot the predicted images\n",
        "        # plot_predictions(images, labels, predictions)\n",
        "        print(time.time() - start_time, \"<<< ⌚ for a batch\")\n",
        "    print(time.time() - start_time, \"<<< ⌛⌛⌛⌛⌛⌛⌛⌛⌛⌛ for a epoch\")\n",
        "# Evaluate the model\n",
        "correct = 0\n",
        "total = 0\n",
        "for batch in data_loader:\n",
        "    images, labels = batch\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    predictions = model(images)\n",
        "    _, predicted = torch.max(predictions.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), \"model.pt\")\n",
        "\n",
        "print(\"The time to completion of the model code is:\", time.time() - start_time)\n"
      ],
      "metadata": {
        "id": "eN-oJxhp8rX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **parallel**\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Nested UNet model\n",
        "model = Nested_UNet(in_ch=3, out_ch=3)\n",
        "\n",
        "# Check if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    # Set the device to GPU\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    # Move the model to GPU\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Print a message to the user\n",
        "    print(\"Model is now running on GPU!\")\n",
        "\n",
        "else:\n",
        "\n",
        "    # Print a message to the user\n",
        "    print(\"GPU is not available!\")\n",
        "\n",
        "# Load the dataset\n",
        "dataset = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
        "\n",
        "# Create a data loader\n",
        "data_loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "import cv2\n",
        "# Define a function to plot the predicted images\n",
        "def plot_predictions(images, labels, predictions):\n",
        "    print(len(images))\n",
        "    for i in range(len(images)):\n",
        "      print(type(predictions[i]), type(images[i]))\n",
        "      \n",
        "      cv2_imshow(cv2.cvtColor(np.array(images[i].cpu().detach().numpy()).reshape(32, 32, 3), cv2.COLOR_BGR2RGB))\n",
        "      cv2_imshow(cv2.cvtColor(np.array(predictions[i].cpu().detach().numpy()).reshape(32, 32, 3), cv2.COLOR_BGR2RGB))\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(10):\n",
        "    for batch in data_loader:\n",
        "        images, labels = batch\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        predictions = model(images)\n",
        "        print(images.shape, predictions.shape)\n",
        "        loss = nn.CrossEntropyLoss()(predictions, images)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Plot the predicted images\n",
        "        # plot_predictions(images, labels, predictions)\n",
        "\n",
        "# Evaluate the model\n",
        "correct = 0\n",
        "total = 0\n",
        "for batch in data_loader:\n",
        "    images, labels = batch\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    predictions = model(images)\n",
        "    _, predicted = torch.max(predictions.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), \"model.pt\")\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "tgkwetek4esU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}