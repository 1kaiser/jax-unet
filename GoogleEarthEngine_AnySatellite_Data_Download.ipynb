{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1kaiser/jax-unet/blob/master/GoogleEarthEngine_AnySatellite_Data_Download.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fy4HceOol_h1",
        "outputId": "8e236db8-0c34-4a5e-a53c-9100369b895d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Install the library\n",
        "!pip -q install FireHR==0.1.2 pyhdf==0.10.2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/149.1 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m149.1/149.1 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.3/193.3 KB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.8/64.8 KB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m776.8/776.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.0/56.0 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m209.2/209.2 KB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.1/16.1 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.1/58.1 KB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m82.1/82.1 KB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.5/64.5 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.3/64.3 KB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.1/64.1 KB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.8/63.8 KB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.5/63.5 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.0/64.0 KB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.3/63.3 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.1/63.1 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.7/63.7 KB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.1/63.1 KB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.1/63.1 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.0/63.0 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.0/62.0 KB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.9/61.9 KB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.6/61.6 KB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.9/65.9 KB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.8/62.8 KB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.4/64.4 KB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.4/64.4 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.4/64.4 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.1/64.1 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.1/64.1 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.2/64.2 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.0/57.0 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.9/56.9 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.9/56.9 KB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.0/57.0 KB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.1/54.1 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.3/53.3 KB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.7/51.7 KB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.3/49.3 KB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.3/64.3 KB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.2/49.2 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.5/48.5 KB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.6/47.6 KB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.2/47.2 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.0/47.0 KB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.9/46.9 KB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.9/46.9 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.9/46.9 KB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.6/50.6 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.4/49.4 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.5/50.5 KB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.3/49.3 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m133.6/133.6 KB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.3/49.3 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.3/50.3 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.1/49.1 KB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.1/49.1 KB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.1/49.1 KB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.7/57.7 KB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.5/58.5 KB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.3/55.3 KB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.3/55.3 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.3/55.3 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m120.9/120.9 KB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m83.6/83.6 KB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyhdf (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for cdsapi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.7.1 which is incompatible.\n",
            "torchaudio 0.13.1+cu116 requires torch==1.13.1, but you have torch 1.7.1 which is incompatible.\n",
            "en-core-web-sm 3.5.0 requires spacy<3.6.0,>=3.5.0, but you have spacy 2.3.9 which is incompatible.\n",
            "confection 0.0.4 requires srsly<3.0.0,>=2.4.0, but you have srsly 1.0.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKyI9oFmmvpd",
        "outputId": "8fcc3c9e-4ab6-4806-f18d-cd2afdfe57b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Authenticate to use Google Earth Engine API\n",
        "import ee\n",
        "ee.Authenticate()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To authorize access needed by Earth Engine, open the following URL in a web browser and follow the instructions. If the web browser does not start automatically, please manually browse the URL below.\n",
            "\n",
            "    https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=cKHTeetNHyucjvtOQx5Ivzg2VnE2HbvXrfvk1SZMn7A&tc=ioBafxfxJwTgsphvWf9Z-NJmQbU3Md85EZzsrDTtNnE&cc=vO88jnZQcocQQBnJS8fe7msrSP1Y_gPf47xadHIfeb4\n",
            "\n",
            "The authorization workflow will generate a code, which you should paste in the box below.\n",
            "Enter verification code: 4/1AWtgzh5O7qXC4GhSRvwlu_38Z3GPrJ3HfWIeue3U2F_sQ-0drGvUWhaJ0Mc\n",
            "\n",
            "Successfully saved authorization token.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "z87ijnz_4gMy",
        "outputId": "9d314306-59e7-4fe4-d870-65be796f7f57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/OUT/data"
      ],
      "metadata": {
        "id": "IypPoKGP5LrP",
        "outputId": "1c716af8-0ad4-4a98-ab6d-b8aaae558524",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/OUT/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**main code**"
      ],
      "metadata": {
        "id": "C1hnMrqQklVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**bare code**"
      ],
      "metadata": {
        "id": "_bmbtllFy7KQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "import ee\n",
        "import os\n",
        "import requests\n",
        "import rasterio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import json\n",
        "from IPython.core.debugger import set_trace\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "from fastprogress.fastprogress import progress_bar\n",
        "from banet.geo import open_tif, merge, Region\n",
        "from banet.geo import downsample\n",
        "     \n",
        "from FireHR.data import *\n",
        "#export\n",
        "class RegionST(Region):\n",
        "    \"Defines a region in space and time with a name, a bounding box and the pixel size.\"\n",
        "    def __init__(self, name:str, bbox:list, pixel_size:float=None, scale_meters:int=None,\n",
        "                 time_start:str=None, time_end:str=None, time_freq:str='D', time_margin:int=0,\n",
        "                 shape:tuple=None, epsg=4326):\n",
        "        if scale_meters is not None and pixel_size is not None: \n",
        "            raise Exception('Either pixel_size or scale_meters must be set to None.')\n",
        "        self.name = name\n",
        "        self.bbox = rasterio.coords.BoundingBox(*bbox) # left, bottom, right, top\n",
        "        if pixel_size is not None:\n",
        "            self.pixel_size = pixel_size\n",
        "        else:\n",
        "            self.pixel_size = scale_meters/111000\n",
        "        self.epsg         = epsg\n",
        "        self.scale_meters = scale_meters\n",
        "        self._shape       = shape\n",
        "        self.time_start   = pd.Timestamp(str(time_start))\n",
        "        self.time_end     = pd.Timestamp(str(time_end))\n",
        "        self.time_margin  = time_margin\n",
        "        self.time_freq    = time_freq\n",
        "\n",
        "    @property\n",
        "    def shape(self):\n",
        "        \"Shape of the region (height, width)\"\n",
        "        if self._shape is None:\n",
        "            return (self.height, self.width)\n",
        "        else: return self._shape\n",
        "        \n",
        "    @property\n",
        "    def times(self):\n",
        "        \"Property that computes the date_range for the region.\"\n",
        "        tstart = self.time_start - pd.Timedelta(days=self.time_margin)\n",
        "        tend = self.time_end + pd.Timedelta(days=self.time_margin)\n",
        "        return pd.date_range(tstart, tend, freq=self.time_freq)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, file, time_start=None, time_end=None):\n",
        "        \"Loads region information from json file\"\n",
        "        with open(file, 'r') as f:\n",
        "            args = json.load(f)\n",
        "        if time_start is None:\n",
        "            time_start = args['time_start']\n",
        "        if time_end is None:\n",
        "            time_end = args['time_end']\n",
        "        return cls(args['name'], args['bbox'], args['pixel_size'],\n",
        "                   time_start=time_start, time_end=time_end)\n",
        "    \n",
        "def extract_region(df_row, cls=Region):\n",
        "    \"Create Region object from a row of the metadata dataframe.\"\n",
        "    if issubclass(cls, RegionST):\n",
        "        return cls(df_row.event_id, df_row.bbox, df_row.pixel_size, \n",
        "                   df_row.time_start, df_row.time_end)\n",
        "    elif issubclass(cls, Region):\n",
        "        return cls(df_row.event_id, df_row.bbox, df_row.pixel_size)\n",
        "    else: raise NotImplemented('cls must be one of the following [Region, RegionST]')\n",
        "     \n",
        "\n",
        "#export\n",
        "def coords2bbox(lon, lat, pixel_size): \n",
        "    return [lon.min(), lat.min(), lon.max()+pixel_size, lat.max()+pixel_size]\n",
        "\n",
        "def split_region(region:RegionST, size:int, cls=Region):\n",
        "    lon, lat = region.coords()\n",
        "    Nlon = (len(lon)//size)*size\n",
        "    Nlat = (len(lat)//size)*size\n",
        "    lons = [*lon[:Nlon].reshape(-1, size), lon[Nlon:][None]]\n",
        "    lats = [*lat[:Nlat].reshape(-1, size), lat[Nlat:][None]]\n",
        "    if len(lats[-1].reshape(-1)) == 0 and len(lons[-1].reshape(-1)) == 0:\n",
        "        lons = lons[:-1]\n",
        "        lats = lats[:-1]\n",
        "    #lons = lon.reshape(-1, size)\n",
        "    #lats = lat.reshape(-1, size)\n",
        "    if issubclass(cls, RegionST):\n",
        "        return [cls('', coords2bbox(ilon, ilat, region.pixel_size), \n",
        "                    pixel_size=region.pixel_size, time_start=region.time_start,\n",
        "                    time_end=region.time_end, time_freq=region.time_freq,\n",
        "                    time_margin=region.time_margin) for ilon in lons for ilat in lats]\n",
        "    elif issubclass(cls, Region):\n",
        "        return [cls('', coords2bbox(ilon, ilat, region.pixel_size), pixel_size=region.pixel_size) \n",
        "            for ilon in lons for ilat in lats]\n",
        "    else: raise NotImplemented('cls must be one of the following [Region, RegionST]')\n",
        "        \n",
        "    return \n",
        "            \n",
        "def merge_tifs(files:list, fname:str, delete=False):\n",
        "    data, tfm = merge([open_tif(str(f)) for f in files])\n",
        "    data = data.squeeze()\n",
        "    fname = Path(files[0]).parent/fname\n",
        "    profile = open_tif(str(files[0])).profile\n",
        "    with rasterio.Env():\n",
        "        height, width = data.shape\n",
        "        profile.update(width=width, height=height, transform=tfm, compress='lzw')\n",
        "        with rasterio.open(str(fname), 'w', **profile) as dst:\n",
        "            dst.write(data, 1)\n",
        "    if delete:\n",
        "        for f in files: os.remove(f)\n",
        "     \n",
        "\n",
        "#export\n",
        "def filter_region(image_collection:ee.ImageCollection, region:RegionST, times:tuple, bands=None):\n",
        "    image_collection = image_collection.filterDate(times[0], times[1])\n",
        "    geometry = ee.Geometry.Rectangle(region.bbox)\n",
        "    image_collection = image_collection.filterBounds(geometry)\n",
        "    if bands is not None:\n",
        "        image_collection = image_collection.select(bands)\n",
        "    return image_collection\n",
        "\n",
        "def filter_cloudy(image_collection:ee.ImageCollection, max_cloud_fraction=0.2):\n",
        "    return image_collection.filterMetadata(\n",
        "        'CLOUDY_PIXEL_PERCENTAGE', 'not_greater_than', max_cloud_fraction)\n",
        "\n",
        "def n_least_cloudy(image_collection:ee.ImageCollection, n=5):\n",
        "    image_collection = image_collection.sort(prop='CLOUDY_PIXEL_PERCENTAGE')\n",
        "    image_collection = image_collection.toList(image_collection.size())\n",
        "    colsize = image_collection.size().getInfo()\n",
        "    if colsize < n: \n",
        "        warnings.warn(f'Total number of images in the collection {colsize} less than n={n}. Setting n={colsize}')\n",
        "        n = colsize\n",
        "    image_collection = ee.ImageCollection([ee.Image(image_collection.get(i)) for i in range(n)])\n",
        "    return image_collection\n",
        "\n",
        "def download_topography_data(R:RegionST, path_save=Path('.'), scale=None, \n",
        "                             download_crop_size=1000, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    image = ee.Image('srtm90_v4')\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size)\n",
        "    if not (path_save/'srtm90_v4.elevation.tif').is_file():\n",
        "        files = []\n",
        "        loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "        for j, R in loop:\n",
        "            region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" +\n",
        "              f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "            url = image.getDownloadUrl(\n",
        "                {'scale': scale, 'crs': 'EPSG:4326', 'region': f'{region}'})\n",
        "            r = requests.get(url)\n",
        "            with open(str(path_save/'data.zip'), 'wb') as f:\n",
        "                f.write(r.content)\n",
        "            with zipfile.ZipFile(str(path_save/'data.zip'), 'r') as f:\n",
        "                f.extractall(str(path_save))\n",
        "                os.rename(str(path_save/'srtm90_v4.elevation.tif'),\n",
        "                          str(path_save/f'srtm90_v4.elevation_{j}.tif'))\n",
        "                files.append(str(path_save/f'srtm90_v4.elevation_{j}.tif'))\n",
        "            os.remove(str(path_save/'data.zip'))\n",
        "        merge_tifs(files, 'srtm90_v4.elevation.tif', delete=True)\n",
        "\n",
        "def download_data(R:RegionST, times, products, bands, path_save, scale=None, max_cloud_fraction=None,\n",
        "                  use_least_cloudy=None, download_crop_size=1000, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    if not ((path_save/f'download.{bands[0]}.tif').is_file() and \n",
        "           (path_save/f'download.{bands[1]}.tif').is_file() and\n",
        "           (path_save/f'download.{bands[2]}.tif').is_file()):\n",
        "        sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size, cls=RegionST)\n",
        "        fsaves = []\n",
        "        #for j, R in tqdm(enumerate(sR), total=len(sR)):\n",
        "        loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "        for j, R in loop:\n",
        "            region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" +\n",
        "                       f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "\n",
        "            if not ((path_save/f'download.{bands[0]}_{j}.tif').is_file() and \n",
        "                   (path_save/f'download.{bands[1]}_{j}.tif').is_file() and\n",
        "                   (path_save/f'download.{bands[2]}_{j}.tif').is_file()):\n",
        "                # Merge products to single image collection\n",
        "                imCol = ee.ImageCollection(products[0])\n",
        "                for i in range(1, len(products)):\n",
        "                    imCol = imCol.merge(ee.ImageCollection(products[i]))\n",
        "                imCol = filter_region(imCol, R, times=times, bands=bands)\n",
        "                if max_cloud_fraction is not None:\n",
        "                    imCol = filter_cloudy(imCol, max_cloud_fraction=max_cloud_fraction)\n",
        "                if use_least_cloudy is not None:\n",
        "                    imCol = n_least_cloudy(imCol, n=use_least_cloudy)\n",
        "                im = imCol.median()\n",
        "                imCol = ee.ImageCollection([im])\n",
        "                colList = imCol.toList(imCol.size())\n",
        "                # info = colList.getInfo()\n",
        "                # data_times = [pd.to_datetime(o['properties']['system:time_start'], unit='ms') for o in info]\n",
        "                # data_cloudy = [o['properties']['CLOUDY_PIXEL_PERCENTAGE'] for o in info]\n",
        "                # Download each image\n",
        "                for i in range(colList.size().getInfo()):\n",
        "                    image = ee.Image(colList.get(i))\n",
        "                    fname = 'download'\n",
        "                    #fname = image.get('system:id').getInfo().split('/')[-1]\n",
        "                    fnames_full = [f'{fname}.{b}.tif' for b in bands]\n",
        "                    fnames_partial0 = [f'{fname}.{b}_{j}.tif' for b in bands]\n",
        "                    fnames_full = all([(path_save/f).is_file() for f in fnames_full])\n",
        "                    fnames_partial = all([(path_save/f).is_file() for f in fnames_partial0])\n",
        "                    if not fnames_full:\n",
        "                        fsaves.append([path_save/f for f in fnames_partial0])\n",
        "                        if not fnames_partial:\n",
        "                            zip_error = True\n",
        "                            for i in range(10): # Try 10 times\n",
        "                                if zip_error:\n",
        "                                    try:\n",
        "                                        url = image.getDownloadURL({'scale': scale, 'crs': 'EPSG:4326', 'region': f'{region}'})\n",
        "                                        r = requests.get(url)\n",
        "                                        with open(str(path_save/'data.zip'), 'wb') as f:\n",
        "                                            f.write(r.content)\n",
        "                                        with zipfile.ZipFile(str(path_save/'data.zip'), 'r') as f:\n",
        "                                            files = f.namelist()\n",
        "                                            f.extractall(str(path_save))\n",
        "                                        os.remove(str(path_save/'data.zip'))\n",
        "                                        zip_error = False\n",
        "                                    except:\n",
        "                                        zip_error = True\n",
        "                                        os.remove(str(path_save/'data.zip'))\n",
        "                                        time.sleep(10)\n",
        "                            if zip_error: raise Exception(f'Failed to process {url}')\n",
        "                            for f in files:\n",
        "                                f = path_save/f\n",
        "                                os.rename(str(f), str(path_save/f'{f.stem}_{j}{f.suffix}'))\n",
        "        # Merge files\n",
        "        suffix = '.tif'\n",
        "        files = path_save.ls(include=[suffix])\n",
        "        #files = np.unique(fsaves) \n",
        "        files = [o.stem for o in files]\n",
        "        ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 6])\n",
        "        ids = np.unique([int(o.split('_')[-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 6])\n",
        "        #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "        file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                    if f'{r}_{i}' in files] for r in ref] \n",
        "        for fs in file_groups:\n",
        "            if len(fs) < 500:\n",
        "                fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "                merge_tifs(fs, fsave, delete=True)\n",
        "            else:\n",
        "                fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
        "                if len(fs[(len(fs)//500)*500:]) > 0:\n",
        "                    fs_break.append(fs[(len(fs)//500)*500:])\n",
        "                for fsi, fs2 in enumerate(fs_break):\n",
        "                    fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
        "                    merge_tifs(fs2, fsave, delete=True)\n",
        "\n",
        "        files = path_save.ls(include=[suffix, '_break'])\n",
        "        files = [o.stem for o in files]\n",
        "        ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 11])\n",
        "        ids = np.unique([o.split('_')[-1]\n",
        "                         for o in files if len(o.split('_')[-1]) < 11])\n",
        "        #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "        file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                    if f'{r}_{i}' in files] for r in ref] \n",
        "        for fs in file_groups:\n",
        "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "            merge_tifs(fs, fsave, delete=True)\n",
        "            \n",
        "def download_data_ts(R:RegionST, products, bands, path_save, scale=None, \n",
        "                     download_crop_size=1000, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    times = (R.times[0], R.times[-1])\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size, cls=RegionST)\n",
        "    loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "    for j, R in loop:\n",
        "        region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" + f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "\n",
        "        # Merge products to single image collection\n",
        "        imCol = ee.ImageCollection(products[0])\n",
        "        for i in range(1, len(products)):\n",
        "            imCol = imCol.merge(ee.ImageCollection(products[i]))\n",
        "        imCol = filter_region(imCol, R, times=times, bands=bands)\n",
        "        imCol = ee.ImageCollection(imCol)\n",
        "        colList = imCol.toList(imCol.size())\n",
        "\n",
        "        # Download each image\n",
        "        for i in range(colList.size().getInfo()):\n",
        "            image = ee.Image(colList.get(i))\n",
        "            zip_error = True\n",
        "            for i in range(10): # Try 10 times\n",
        "                if zip_error:\n",
        "                    try:\n",
        "                        url = image.getDownloadURL({'scale': scale, 'crs': 'EPSG:4326','region': f'{region}'})\n",
        "                        r = requests.get(url)\n",
        "                        with open(str(path_save/'data.zip'), 'wb') as f:\n",
        "                            f.write(r.content)\n",
        "                        with zipfile.ZipFile(str(path_save/'data.zip'), 'r') as f:\n",
        "                            files = f.namelist()\n",
        "                            f.extractall(str(path_save))\n",
        "                        os.remove(str(path_save/'data.zip'))\n",
        "                        zip_error = False\n",
        "                    except:\n",
        "                        zip_error = True\n",
        "                        os.remove(str(path_save/'data.zip'))\n",
        "                        time.sleep(10)\n",
        "            if zip_error: raise Exception(f'Failed to process {url}')\n",
        "            for f in files:\n",
        "                f = path_save/f\n",
        "                os.rename(str(f), str(path_save/f'{f.stem}_{j}{f.suffix}'))\n",
        "                \n",
        "    # Merge files\n",
        "    suffix = '.tif'\n",
        "    files = path_save.ls(include=[suffix])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 6])\n",
        "    ids = np.unique([int(o.split('_')[-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 6])\n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        if len(fs) < 500:\n",
        "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "            merge_tifs(fs, fsave, delete=True)\n",
        "        else:\n",
        "            fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
        "            if len(fs[(len(fs)//500)*500:]) > 0:\n",
        "                fs_break.append(fs[(len(fs)//500)*500:])\n",
        "            for fsi, fs2 in enumerate(fs_break):\n",
        "                fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
        "                merge_tifs(fs2, fsave, delete=True)\n",
        "\n",
        "    files = path_save.ls(include=[suffix, '_break'])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 11])\n",
        "    ids = np.unique([o.split('_')[-1]\n",
        "                     for o in files if len(o.split('_')[-1]) < 11])\n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "        merge_tifs(fs, fsave, delete=True)"
      ],
      "metadata": {
        "id": "cgDoL5qty-Mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**COPERNICUS/S2**"
      ],
      "metadata": {
        "id": "qaYr__CZrM7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "import ee\n",
        "import os\n",
        "import requests\n",
        "import rasterio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import json\n",
        "from IPython.core.debugger import set_trace\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "from fastprogress.fastprogress import progress_bar\n",
        "from banet.geo import open_tif, merge, Region\n",
        "from banet.geo import downsample\n",
        "   \n",
        "   \n",
        "import requests\n",
        "import time\n",
        "from multiprocessing import cpu_count\n",
        "from multiprocessing.pool import ThreadPool\n",
        "from google.colab import output\n",
        "from FireHR.data import *\n",
        "    \n",
        "cpus = cpu_count()\n",
        "url_list = []\n",
        "\n",
        "###########################################################################################################################\n",
        "def download_url(url):\n",
        "  r = requests.get(url)\n",
        "  with open(str(path_save/'data.zip'), 'wb') as f:\n",
        "      f.write(r.content)\n",
        "  with zipfile.ZipFile(str(path_save/'data.zip'), 'r') as f:\n",
        "      files = f.namelist()\n",
        "      f.extractall(str(path_save))\n",
        "  os.remove(str(path_save/'data.zip'))\n",
        "  print(\"downloading ğŸ”»:\",url_list.index(url),\"/\",len(url_list))\n",
        "  for f in files:\n",
        "      f = path_save/f\n",
        "      os.rename(str(f), str(path_save/f'{f.stem}_{url_list.index(url)}{f.suffix}'))\n",
        "###########################################################################################################################\n",
        "        \n",
        "\n",
        "def download_parallel(args):\n",
        "    cpus = cpu_count()\n",
        "    results = ThreadPool(cpus - 1).imap_unordered(download_url, args)\n",
        "    for result in results:\n",
        "        print('', result)\n",
        "        output.clear()\n",
        "###########################################################################################################################\n",
        "\n",
        "# // Applies scaling factors.\n",
        "def applyScaleFactors_COPERNICUS(image):\n",
        "  opticalBands = image.divide(10000);\n",
        "  return image.addBands(srcImg = opticalBands, overwrite = True)\n",
        "\n",
        "\n",
        "\n",
        "def download_data_ts_COPERNICUS(R:RegionST, products, bands, path_save, scale=None, download_crop_size=500, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    times = (R.times[0], R.times[-1])\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size, cls=RegionST)\n",
        "    loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "    url_list.clear()\n",
        "    for j, R in loop:\n",
        "      region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" + f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "\n",
        "      # Merge products to single image collection\n",
        "      imCol = ee.ImageCollection(products[0])\n",
        "      imCol = filter_region(imCol, R, times=times, bands=bands)\n",
        "      imCol = ee.ImageCollection(imCol).map(applyScaleFactors_COPERNICUS)\n",
        "      colList = imCol.toList(imCol.size())\n",
        "  \n",
        "      # Download each image\n",
        "      for i in range(colList.size().getInfo()):\n",
        "          image = ee.Image(colList.get(i))\n",
        "          url_list.append(str(image.getDownloadURL({'scale': scale, 'crs': 'EPSG:4326','region': f'{region}'})))\n",
        "\n",
        "\n",
        "    #download function in parallel fashion from collected urls\n",
        "    download_parallel(url_list)   \n",
        "\n",
        "    # Merge files\n",
        "    suffix = '.tif'\n",
        "    files = path_save.ls(include=[suffix])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 6])\n",
        "    ids = np.unique([int(o.split('_')[-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 6])\n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        if len(fs) < 500:\n",
        "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "            merge_tifs(fs, fsave, delete=True)\n",
        "        else:\n",
        "            fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
        "            if len(fs[(len(fs)//500)*500:]) > 0:\n",
        "                fs_break.append(fs[(len(fs)//500)*500:])\n",
        "            for fsi, fs2 in enumerate(fs_break):\n",
        "                fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
        "                merge_tifs(fs2, fsave, delete=True)\n",
        "\n",
        "    files = path_save.ls(include=[suffix, '_break'])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 11])\n",
        "    ids = np.unique([o.split('_')[-1]\n",
        "                     for o in files if len(o.split('_')[-1]) < 11])\n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "        merge_tifs(fs, fsave, delete=True)\n",
        "\n",
        "\n",
        "def download_data_COPERNICUS(R:RegionST, times, products, bands, path_save, scale=None, max_cloud_fraction=None, use_least_cloudy=None, download_crop_size=500, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    if not ((path_save/f'download.{bands[0]}.tif').is_file()):\n",
        "        sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size, cls=RegionST)\n",
        "        fsaves = []\n",
        "        #for j, R in tqdm(enumerate(sR), total=len(sR)):\n",
        "        loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "        url_list.clear()\n",
        "        for j, R in loop:\n",
        "            region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" + f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "\n",
        "            if not ((path_save/f'download.{bands[0]}_{j}.tif').is_file()):\n",
        "                # Merge products to single image collection\n",
        "                imCol = ee.ImageCollection(products[0])\n",
        "                imCol = filter_region(imCol, R, times=times, bands=bands)\n",
        "                imCol = ee.ImageCollection(imCol).map(applyScaleFactors_COPERNICUS)\n",
        "                if max_cloud_fraction is not None:\n",
        "                    imCol = filter_cloudy(imCol, max_cloud_fraction=max_cloud_fraction)\n",
        "                if use_least_cloudy is not None:\n",
        "                    imCol = n_least_cloudy(imCol, n=use_least_cloudy)\n",
        "                im = imCol.median()\n",
        "                imCol = ee.ImageCollection([im])\n",
        "                colList = imCol.toList(imCol.size())\n",
        "                # info = colList.getInfo()\n",
        "                # data_times = [pd.to_datetime(o['properties']['system:time_start'], unit='ms') for o in info]\n",
        "                # data_cloudy = [o['properties']['CLOUDY_PIXEL_PERCENTAGE'] for o in info]\n",
        "                # Download each image\n",
        "                for i in range(colList.size().getInfo()):\n",
        "                    image = ee.Image(colList.get(i))\n",
        "                    fname = 'download'\n",
        "                    #fname = image.get('system:id').getInfo().split('/')[-1]\n",
        "                    fnames_full = [f'{fname}.{b}.tif' for b in bands]\n",
        "                    fnames_partial0 = [f'{fname}.{b}_{j}.tif' for b in bands]\n",
        "                    fnames_full = all([(path_save/f).is_file() for f in fnames_full])\n",
        "                    fnames_partial = all([(path_save/f).is_file() for f in fnames_partial0])\n",
        "                    if not fnames_full:\n",
        "                        fsaves.append([path_save/f for f in fnames_partial0])\n",
        "                        if not fnames_partial:\n",
        "                          url_list.append(str(image.getDownloadURL({'scale': scale, 'crs': 'EPSG:4326','region': f'{region}'})))\n",
        "\n",
        "        #download function in parallel fashion from collected urls\n",
        "        download_parallel(url_list) \n",
        "\n",
        "        # Merge files\n",
        "        suffix = '.tif'\n",
        "        files = path_save.ls(include=[suffix])\n",
        "        #files = np.unique(fsaves) \n",
        "        files = [o.stem for o in files]\n",
        "        ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 6])\n",
        "        ids = np.unique([int(o.split('_')[-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 6])\n",
        "        #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "        file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                    if f'{r}_{i}' in files] for r in ref] \n",
        "        for fs in file_groups:\n",
        "            if len(fs) < 500:\n",
        "                fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "                merge_tifs(fs, fsave, delete=True)\n",
        "            else:\n",
        "                fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
        "                if len(fs[(len(fs)//500)*500:]) > 0:\n",
        "                    fs_break.append(fs[(len(fs)//500)*500:])\n",
        "                for fsi, fs2 in enumerate(fs_break):\n",
        "                    fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
        "                    merge_tifs(fs2, fsave, delete=True)\n",
        "\n",
        "        files = path_save.ls(include=[suffix, '_break'])\n",
        "        files = [o.stem for o in files]\n",
        "        ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 11])\n",
        "        ids = np.unique([o.split('_')[-1]\n",
        "                         for o in files if len(o.split('_')[-1]) < 11])\n",
        "        #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "        file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                    if f'{r}_{i}' in files] for r in ref] \n",
        "        for fs in file_groups:\n",
        "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "            merge_tifs(fs, fsave, delete=True)"
      ],
      "metadata": {
        "id": "uz_pX_6Dw0Ji"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-rR0f-GmELJ",
        "outputId": "49e2b41a-f6ff-4908-caa8-4def9675b6a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "source": [
        "from pathlib import Path\n",
        "from FireHR.data import *\n",
        "\n",
        "# # Bounding box coordinates BEAS BASIN\n",
        "# left   = 75.979728\n",
        "# right  = 77.866667\n",
        "# bottom = 31.453599\n",
        "# top    = 32.416667\n",
        "\n",
        "# Bounding box coordinates for iCEsat 2\n",
        "left   = 77.301555 \n",
        "right  = 77.637165 \n",
        "bottom = 31.779603 \n",
        "top    = 32.041187 \n",
        "\n",
        "path_save   = Path('data')\n",
        "products    = [\"COPERNICUS/S2\"]  # Product id in google earth engine\n",
        "bands       = ['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B11', 'B12'] # Red, Green, Blue\n",
        "\n",
        "R = RegionST(name         = 'TeslaGigaBerlin', \n",
        "             bbox         = [left,bottom,right,top], \n",
        "             scale_meters = 10, \n",
        "             time_start   = '2019-03-03', \n",
        "             time_end     = '2019-05-26')\n",
        "\n",
        "# Download time series\n",
        "# download_data_ts(R, products, bands, path_save)\n",
        "\n",
        "time_window = R.times[0], R.times[-1]\n",
        "\n",
        "# Download median composite of the 3 least cloudy images within the time_window\n",
        "# download_data_COPERNICUS(R, time_window, products, bands, path_save, use_least_cloudy=3, show_progress=True)\n",
        "\n",
        "download_data_ts_COPERNICUS(R, products, bands, path_save, show_progress=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='12' class='' max='924' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      1.30% [12/924 02:38&lt;3:21:21]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-7b1d6479a265>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# download_data_COPERNICUS(R, time_window, products, bands, path_save, use_least_cloudy=3, show_progress=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mdownload_data_ts_COPERNICUS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproducts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbands\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_save\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-8f72362b0316>\u001b[0m in \u001b[0;36mdownload_data_ts_COPERNICUS\u001b[0;34m(R, products, bands, path_save, scale, download_crop_size, show_progress)\u001b[0m\n\u001b[1;32m     77\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m           \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mee\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m           \u001b[0murl_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetDownloadURL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'scale'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'crs'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'EPSG:4326'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'region'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf'{region}'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/ee/image.py\u001b[0m in \u001b[0;36mgetDownloadURL\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0mrequest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0mrequest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakeDownloadUrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetDownloadId\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mgetThumbId\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/ee/data.py\u001b[0m in \u001b[0;36mgetDownloadId\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m   1139\u001b[0m   }\n\u001b[1;32m   1140\u001b[0m   \u001b[0m_maybe_populate_workload_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueryParams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m   result = _execute_cloud_call(\n\u001b[0m\u001b[1;32m   1142\u001b[0m       _get_cloud_api_resource().projects().thumbnails().create(\n\u001b[1;32m   1143\u001b[0m           parent=_get_projects_path(), **queryParams))\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/ee/data.py\u001b[0m in \u001b[0;36m_execute_cloud_call\u001b[0;34m(call, num_retries)\u001b[0m\n\u001b[1;32m    327\u001b[0m   \"\"\"\n\u001b[1;32m    328\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_retries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_retries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mgoogleapiclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHttpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0m_translate_cloud_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/googleapiclient/_helpers.py\u001b[0m in \u001b[0;36mpositional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mpositional_parameters_enforcement\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPOSITIONAL_WARNING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpositional_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/googleapiclient/http.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;31m# Handle retries for server-side errors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 923\u001b[0;31m         resp, content = _retry_request(\n\u001b[0m\u001b[1;32m    924\u001b[0m             \u001b[0mhttp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0mnum_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/googleapiclient/http.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(http, num_retries, req_type, sleep, rand, uri, method, *args, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0;31m# Retry on SSL errors and socket timeout errors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_ssl_SSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mssl_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/google_auth_httplib2.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, uri, method, body, headers, redirections, connection_type, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;31m# Make the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         response, content = self.http.request(\n\u001b[0m\u001b[1;32m    219\u001b[0m             \u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/ee/_cloud_api_utils.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m       \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_redirects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mredirections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m       response = session.request(\n\u001b[0m\u001b[1;32m     63\u001b[0m           method, uri, data=body, headers=headers, timeout=self._timeout)\n\u001b[1;32m     64\u001b[0m       \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    527\u001b[0m         }\n\u001b[1;32m    528\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                 resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    441\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m             httplib_response = self._make_request(\n\u001b[0m\u001b[1;32m    704\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    442\u001b[0m                 \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1375\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1377\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1378\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1240\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1242\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1243\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBTGABv5sm0d",
        "outputId": "93888446-aedb-47e1-bec7-f1bda567602e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from banet.data import open_tif\n",
        "\n",
        "brightness = 3\n",
        "im = np.concatenate([open_tif(f'data/download.{b}.tif').read() for b in bands])\n",
        "im = im.transpose(1,2,0).astype(np.float32)/10000\n",
        "plt.imshow(brightness*im)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-e03288ae6098>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbrightness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mopen_tif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'data/download.{b}.tif'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbands\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbrightness\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-e03288ae6098>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbrightness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mopen_tif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'data/download.{b}.tif'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbands\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbrightness\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**GEDI**"
      ],
      "metadata": {
        "id": "Namgq3XVrIgh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**gedi code**"
      ],
      "metadata": {
        "id": "FFi8USYVvogC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "import ee\n",
        "import os\n",
        "import requests\n",
        "import rasterio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import json\n",
        "from IPython.core.debugger import set_trace\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "from fastprogress.fastprogress import progress_bar\n",
        "from banet.geo import open_tif, merge, Region\n",
        "from banet.geo import downsample\n",
        "   \n",
        "   \n",
        "import requests\n",
        "import time\n",
        "from multiprocessing import cpu_count\n",
        "from multiprocessing.pool import ThreadPool\n",
        "from google.colab import output\n",
        "from FireHR.data import *\n",
        "   \n",
        "cpus = cpu_count()\n",
        "url_list = []\n",
        "\n",
        "###########################################################################################################################\n",
        "def download_url(url):\n",
        "  r = requests.get(url)\n",
        "  with open(str(path_save/'data.zip'), 'wb') as f:\n",
        "      f.write(r.content)\n",
        "  with zipfile.ZipFile(str(path_save/'data.zip'), 'r') as f:\n",
        "      files = f.namelist()\n",
        "      f.extractall(str(path_save))\n",
        "  os.remove(str(path_save/'data.zip'))\n",
        "  print(\"downloading ğŸ”»:\",url_list.index(url),\"/\",len(url_list))\n",
        "  for f in files:\n",
        "      f = path_save/f\n",
        "      os.rename(str(f), str(path_save/f'{f.stem}_{url_list.index(url)}{f.suffix}'))\n",
        "###########################################################################################################################\n",
        "        \n",
        "\n",
        "def download_parallel(args):\n",
        "    cpus = cpu_count()\n",
        "    results = ThreadPool(cpus - 1).imap_unordered(download_url, args)\n",
        "    for result in results:\n",
        "        print('', result)\n",
        "        output.clear()\n",
        "###########################################################################################################################\n",
        "\n",
        "\n",
        "def download_data_GEDI(R:RegionST, times, products, bands, path_save, scale=None, max_cloud_fraction=None,\n",
        "                  use_least_cloudy=None, download_crop_size=1000, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    if not ((path_save/f'download.{bands[0]}.tif').is_file() and \n",
        "           (path_save/f'download.{bands[1]}.tif').is_file() and\n",
        "           (path_save/f'download.{bands[2]}.tif').is_file()):\n",
        "        sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size, cls=RegionST)\n",
        "        fsaves = []\n",
        "        #for j, R in tqdm(enumerate(sR), total=len(sR)):\n",
        "        loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "        url_list.clear()\n",
        "        for j, R in loop:\n",
        "            region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" + f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "\n",
        "            if not ((path_save/f'download.{bands[0]}_{j}.tif').is_file() and \n",
        "                   (path_save/f'download.{bands[1]}_{j}.tif').is_file() and\n",
        "                   (path_save/f'download.{bands[2]}_{j}.tif').is_file()):\n",
        "                # Merge products to single image collection\n",
        "                imCol = ee.ImageCollection(products[0])\n",
        "                imCol = filter_region(imCol, R, times=times, bands=bands)\n",
        "                if max_cloud_fraction is not None:\n",
        "                    imCol = filter_cloudy(imCol, max_cloud_fraction=max_cloud_fraction)\n",
        "                if use_least_cloudy is not None:\n",
        "                    imCol = n_least_cloudy(imCol, n=use_least_cloudy)\n",
        "                im = imCol.median()\n",
        "                imCol = ee.ImageCollection([im])\n",
        "                colList = imCol.toList(imCol.size())\n",
        "                # info = colList.getInfo()\n",
        "                # data_times = [pd.to_datetime(o['properties']['system:time_start'], unit='ms') for o in info]\n",
        "                # data_cloudy = [o['properties']['CLOUDY_PIXEL_PERCENTAGE'] for o in info]\n",
        "                # Download each image\n",
        "                for i in range(colList.size().getInfo()):\n",
        "                    image = ee.Image(colList.get(i))\n",
        "                    fname = 'download'\n",
        "                    #fname = image.get('system:id').getInfo().split('/')[-1]\n",
        "                    fnames_full = [f'{fname}.{b}.tif' for b in bands]\n",
        "                    fnames_partial0 = [f'{fname}.{b}_{j}.tif' for b in bands]\n",
        "                    fnames_full = all([(path_save/f).is_file() for f in fnames_full])\n",
        "                    fnames_partial = all([(path_save/f).is_file() for f in fnames_partial0])\n",
        "                    if not fnames_full:\n",
        "                        fsaves.append([path_save/f for f in fnames_partial0])\n",
        "                        if not fnames_partial:\n",
        "                          url_list.append(str(image.getDownloadURL({'scale': scale, 'crs': 'EPSG:4326','region': f'{region}'})))\n",
        "\n",
        "        #download function in parallel fashion from collected urls\n",
        "        download_parallel(url_list) \n",
        "\n",
        "        # Merge files\n",
        "        suffix = '.tif'\n",
        "        files = path_save.ls(include=[suffix])\n",
        "        #files = np.unique(fsaves) \n",
        "        files = [o.stem for o in files]\n",
        "        ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 6])\n",
        "        ids = np.unique([int(o.split('_')[-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 6])\n",
        "        #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "        file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                    if f'{r}_{i}' in files] for r in ref] \n",
        "        for fs in file_groups:\n",
        "            if len(fs) < 500:\n",
        "                fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "                merge_tifs(fs, fsave, delete=True)\n",
        "            else:\n",
        "                fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
        "                if len(fs[(len(fs)//500)*500:]) > 0:\n",
        "                    fs_break.append(fs[(len(fs)//500)*500:])\n",
        "                for fsi, fs2 in enumerate(fs_break):\n",
        "                    fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
        "                    merge_tifs(fs2, fsave, delete=True)\n",
        "\n",
        "        files = path_save.ls(include=[suffix, '_break'])\n",
        "        files = [o.stem for o in files]\n",
        "        ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 11])\n",
        "        ids = np.unique([o.split('_')[-1]\n",
        "                         for o in files if len(o.split('_')[-1]) < 11])\n",
        "        #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "        file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                    if f'{r}_{i}' in files] for r in ref] \n",
        "        for fs in file_groups:\n",
        "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "            merge_tifs(fs, fsave, delete=True)"
      ],
      "metadata": {
        "id": "JgcwIDqSqzqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/data"
      ],
      "metadata": {
        "id": "qDZFInvyrBva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from FireHR.data import *\n",
        "\n",
        "# Bounding box coordinates\n",
        "left   = 75.979728\n",
        "right  = 77.866667\n",
        "bottom = 31.453599\n",
        "top    = 32.416667\n",
        "\n",
        "path_save   = Path('data')\n",
        "products    = [\"LARSE/GEDI/GEDI02_A_002_MONTHLY\"]  # Product id in google earth engine\n",
        "bands       = ['rh98'] # Red, Green, Blue\n",
        "\n",
        "R = RegionST(name         = 'TeslaGigaBerlin', \n",
        "             bbox         = [left,bottom,right,top], \n",
        "             scale_meters = 10, \n",
        "             time_start   = '2020-04-01', \n",
        "             time_end     = '2020-04-25')\n",
        "\n",
        "# Download time series\n",
        "# download_data_ts(R, products, bands, path_save)\n",
        "\n",
        "time_window = R.times[0], R.times[-1]\n",
        "\n",
        "# Download median composite of the 3 least cloudy images within the time_window\n",
        "download_data_GEDI(R, time_window, products, bands, path_save, use_least_cloudy=3, show_progress=True)\n",
        "\n",
        "#download_data_ts_GEDI(R, products, bands, path_save, show_progress=True)"
      ],
      "metadata": {
        "id": "KOre-TKTrGsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**EO1/HYPERION**"
      ],
      "metadata": {
        "id": "EP8-jdvMdPk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from FireHR.data import *\n",
        "\n",
        "# Bounding box coordinates\n",
        "left   = 75.979728\n",
        "right  = 77.866667\n",
        "bottom = 31.453599\n",
        "top    = 32.416667\n",
        "\n",
        "path_save   = Path('data')\n",
        "products    = [\"EO1/HYPERION\"]  # Product id in google earth engine\n",
        "bands       = ['B104', 'B140', 'B221'] # Red, Green, Blue\n",
        "\n",
        "R = RegionST(name         = 'TeslaGigaBerlin', \n",
        "             bbox         = [left,bottom,right,top], \n",
        "             scale_meters = 10, \n",
        "             time_start   = '2015-03-01', \n",
        "             time_end     = '2016-07-25')\n",
        "\n",
        "# Download time series\n",
        "# download_data_ts(R, products, bands, path_save)\n",
        "\n",
        "time_window = R.times[0], R.times[-1]\n",
        "\n",
        "# Download median composite of the 3 least cloudy images within the time_window\n",
        "download_data(R, time_window, products, bands, path_save, use_least_cloudy=3, show_progress=True)\n",
        "\n",
        "#download_data_ts(R, products, bands, path_save, show_progress=True)"
      ],
      "metadata": {
        "id": "-dgTB5aNdVMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**JAXA/ALOS/PALSAR-2/Level2_2/ScanSAR**"
      ],
      "metadata": {
        "id": "5uoTb-cE4WY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/data"
      ],
      "metadata": {
        "id": "xSM_t-7M1aOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "import ee\n",
        "import os\n",
        "import requests\n",
        "import rasterio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import json\n",
        "from IPython.core.debugger import set_trace\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "from fastprogress.fastprogress import progress_bar\n",
        "from banet.geo import open_tif, merge, Region\n",
        "from banet.geo import downsample\n",
        "   \n",
        "   \n",
        "import requests\n",
        "import time\n",
        "from multiprocessing import cpu_count\n",
        "from multiprocessing.pool import ThreadPool\n",
        "from google.colab import output\n",
        "from FireHR.data import *\n",
        "   \n",
        "cpus = cpu_count()\n",
        "url_list = []\n",
        "\n",
        "###########################################################################################################################\n",
        "def download_url(url):\n",
        "  r = requests.get(url)\n",
        "  with open(str(path_save/'data.zip'), 'wb') as f:\n",
        "      f.write(r.content)\n",
        "  with zipfile.ZipFile(str(path_save/'data.zip'), 'r') as f:\n",
        "      files = f.namelist()\n",
        "      f.extractall(str(path_save))\n",
        "  os.remove(str(path_save/'data.zip'))\n",
        "  print(\"downloading ğŸ”»:\",url_list.index(url),\"/\",len(url_list))\n",
        "  for f in files:\n",
        "      f = path_save/f\n",
        "      os.rename(str(f), str(path_save/f'{f.stem}_{url_list.index(url)}{f.suffix}'))\n",
        "###########################################################################################################################\n",
        "        \n",
        "\n",
        "def download_parallel(args):\n",
        "    cpus = cpu_count()\n",
        "    results = ThreadPool(cpus - 1).imap_unordered(download_url, args)\n",
        "    for result in results:\n",
        "        print('', result)\n",
        "        output.clear()\n",
        "###########################################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def download_data_ts_ALOS(R:RegionST, products, bands, path_save, scale=None, download_crop_size=1000, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    times = (R.times[0], R.times[-1])\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size, cls=RegionST)\n",
        "    loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "    url_list.clear()\n",
        "    for j, R in loop:\n",
        "      region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" + f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "\n",
        "      # Merge products to single image collection\n",
        "      imCol = ee.ImageCollection(products[0])\n",
        "      imCol = filter_region(imCol, R, times=times, bands=bands)\n",
        "      imCol = ee.ImageCollection(imCol)\n",
        "      colList = imCol.toList(imCol.size())\n",
        "  \n",
        "      # Download each image\n",
        "      for i in range(colList.size().getInfo()):\n",
        "          image = ee.Image(colList.get(i))\n",
        "          url_list.append(str(image.getDownloadURL({'scale': scale, 'crs': 'EPSG:4326','region': f'{region}'})))\n",
        "\n",
        "\n",
        "    #download function in parallel fashion from collected urls\n",
        "    download_parallel(url_list)   \n",
        "\n",
        "    # Merge files\n",
        "    suffix = '.tif'\n",
        "    files = path_save.ls(include=[suffix])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 6])\n",
        "    ids = np.unique([int(o.split('_')[-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 6])\n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        if len(fs) < 500:\n",
        "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "            merge_tifs(fs, fsave, delete=True)\n",
        "        else:\n",
        "            fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
        "            if len(fs[(len(fs)//500)*500:]) > 0:\n",
        "                fs_break.append(fs[(len(fs)//500)*500:])\n",
        "            for fsi, fs2 in enumerate(fs_break):\n",
        "                fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
        "                merge_tifs(fs2, fsave, delete=True)\n",
        "\n",
        "    files = path_save.ls(include=[suffix, '_break'])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 11])\n",
        "    ids = np.unique([o.split('_')[-1]\n",
        "                     for o in files if len(o.split('_')[-1]) < 11])\n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "        merge_tifs(fs, fsave, delete=True)\n",
        "\n",
        "\n",
        "def download_data_ALOS(R:RegionST, times, products, bands, path_save, scale=None, max_cloud_fraction=None,\n",
        "                  use_least_cloudy=None, download_crop_size=1000, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    if not ((path_save/f'download.{bands[0]}.tif').is_file()):\n",
        "        sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size, cls=RegionST)\n",
        "        fsaves = []\n",
        "        #for j, R in tqdm(enumerate(sR), total=len(sR)):\n",
        "        loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "        url_list.clear()\n",
        "        for j, R in loop:\n",
        "            region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" + f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "\n",
        "            if not ((path_save/f'download.{bands[0]}_{j}.tif').is_file() and \n",
        "                   (path_save/f'download.{bands[1]}_{j}.tif').is_file() and\n",
        "                   (path_save/f'download.{bands[2]}_{j}.tif').is_file()):\n",
        "                # Merge products to single image collection\n",
        "                imCol = ee.ImageCollection(products[0])\n",
        "                imCol = filter_region(imCol, R, times=times, bands=bands)\n",
        "                if max_cloud_fraction is not None:\n",
        "                    imCol = filter_cloudy(imCol, max_cloud_fraction=max_cloud_fraction)\n",
        "                if use_least_cloudy is not None:\n",
        "                    imCol = n_least_cloudy(imCol, n=use_least_cloudy)\n",
        "                im = imCol.median()\n",
        "                imCol = ee.ImageCollection([im])\n",
        "                colList = imCol.toList(imCol.size())\n",
        "                # info = colList.getInfo()\n",
        "                # data_times = [pd.to_datetime(o['properties']['system:time_start'], unit='ms') for o in info]\n",
        "                # data_cloudy = [o['properties']['CLOUDY_PIXEL_PERCENTAGE'] for o in info]\n",
        "                # Download each image\n",
        "                for i in range(colList.size().getInfo()):\n",
        "                    image = ee.Image(colList.get(i))\n",
        "                    fname = 'download'\n",
        "                    #fname = image.get('system:id').getInfo().split('/')[-1]\n",
        "                    fnames_full = [f'{fname}.{b}.tif' for b in bands]\n",
        "                    fnames_partial0 = [f'{fname}.{b}_{j}.tif' for b in bands]\n",
        "                    fnames_full = all([(path_save/f).is_file() for f in fnames_full])\n",
        "                    fnames_partial = all([(path_save/f).is_file() for f in fnames_partial0])\n",
        "                    if not fnames_full:\n",
        "                        fsaves.append([path_save/f for f in fnames_partial0])\n",
        "                        if not fnames_partial:\n",
        "                          url_list.append(str(image.getDownloadURL({'scale': scale, 'crs': 'EPSG:4326','region': f'{region}'})))\n",
        "\n",
        "        #download function in parallel fashion from collected urls\n",
        "        download_parallel(url_list) \n",
        "\n",
        "        # Merge files\n",
        "        suffix = '.tif'\n",
        "        files = path_save.ls(include=[suffix])\n",
        "        #files = np.unique(fsaves) \n",
        "        files = [o.stem for o in files]\n",
        "        ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 6])\n",
        "        ids = np.unique([int(o.split('_')[-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 6])\n",
        "        #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "        file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                    if f'{r}_{i}' in files] for r in ref] \n",
        "        for fs in file_groups:\n",
        "            if len(fs) < 500:\n",
        "                fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "                merge_tifs(fs, fsave, delete=True)\n",
        "            else:\n",
        "                fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
        "                if len(fs[(len(fs)//500)*500:]) > 0:\n",
        "                    fs_break.append(fs[(len(fs)//500)*500:])\n",
        "                for fsi, fs2 in enumerate(fs_break):\n",
        "                    fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
        "                    merge_tifs(fs2, fsave, delete=True)\n",
        "\n",
        "        files = path_save.ls(include=[suffix, '_break'])\n",
        "        files = [o.stem for o in files]\n",
        "        ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 11])\n",
        "        ids = np.unique([o.split('_')[-1]\n",
        "                         for o in files if len(o.split('_')[-1]) < 11])\n",
        "        #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "        file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                    if f'{r}_{i}' in files] for r in ref] \n",
        "        for fs in file_groups:\n",
        "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "            merge_tifs(fs, fsave, delete=True)"
      ],
      "metadata": {
        "id": "Yr22qb0kBJPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from FireHR.data import *\n",
        "\n",
        "# Bounding box coordinates\n",
        "left   = 75.979728\n",
        "right  = 77.866667\n",
        "bottom = 31.453599\n",
        "top    = 32.416667\n",
        "\n",
        "path_save   = Path('data')\n",
        "products    = [\"JAXA/ALOS/PALSAR-2/Level2_2/ScanSAR\"]  # Product id in google earth engine\n",
        "bands       = ['HH', 'HV'] # Red, Green, Blue\n",
        "\n",
        "R = RegionST(name         = 'TeslaGigaBerlin', \n",
        "             bbox         = [left,bottom,right,top], \n",
        "             scale_meters = 10, \n",
        "             time_start   = '2022-06-07', \n",
        "             time_end     = '2022-06-27')\n",
        "\n",
        "# Download time series\n",
        "# download_data_ts(R, products, bands, path_save)\n",
        "\n",
        "time_window = R.times[0], R.times[-1]\n",
        "\n",
        "# Download median composite of the 3 least cloudy images within the time_window\n",
        "# download_data_ALOS(R, time_window, products, bands, path_save, use_least_cloudy=3, show_progress=True)\n",
        "\n",
        "download_data_ts_ALOS(R, products, bands, path_save, show_progress=True)"
      ],
      "metadata": {
        "id": "3-giNuSezZxi",
        "outputId": "b2873480-d017-4c70-e255-6acc1a1ac8d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='44' class='' max='231' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      19.05% [44/231 00:39&lt;02:47]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**LANDSAT/LC08/C02/T2_L2**"
      ],
      "metadata": {
        "id": "DsgIikSUjWGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "import ee\n",
        "import os\n",
        "import requests\n",
        "import rasterio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import json\n",
        "from IPython.core.debugger import set_trace\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "from fastprogress.fastprogress import progress_bar\n",
        "from banet.geo import open_tif, merge, Region\n",
        "from banet.geo import downsample\n",
        "   \n",
        "   \n",
        "import requests\n",
        "import time\n",
        "from multiprocessing import cpu_count\n",
        "from multiprocessing.pool import ThreadPool\n",
        "from google.colab import output\n",
        "from FireHR.data import *\n",
        "    \n",
        "cpus = cpu_count()\n",
        "url_list = []\n",
        "\n",
        "###########################################################################################################################\n",
        "def download_url(url):\n",
        "  r = requests.get(url)\n",
        "  with open(str(path_save/'data.zip'), 'wb') as f:\n",
        "      f.write(r.content)\n",
        "  with zipfile.ZipFile(str(path_save/'data.zip'), 'r') as f:\n",
        "      files = f.namelist()\n",
        "      f.extractall(str(path_save))\n",
        "  os.remove(str(path_save/'data.zip'))\n",
        "  print(\"downloading ğŸ”»:\",url_list.index(url),\"/\",len(url_list))\n",
        "  for f in files:\n",
        "      f = path_save/f\n",
        "      os.rename(str(f), str(path_save/f'{f.stem}_{url_list.index(url)}{f.suffix}'))\n",
        "###########################################################################################################################\n",
        "        \n",
        "\n",
        "def download_parallel(args):\n",
        "    cpus = cpu_count()\n",
        "    results = ThreadPool(cpus - 1).imap_unordered(download_url, args)\n",
        "    for result in results:\n",
        "        print('', result)\n",
        "        output.clear()\n",
        "###########################################################################################################################\n",
        "\n",
        "# // Applies scaling factors.\n",
        "def applyScaleFactors_LANDSAT(image):\n",
        "  opticalBands = image.select('SR_B.').multiply(0.0000275).add(-0.2);\n",
        "  thermalBands = image.select('ST_B.*').multiply(0.00341802).add(149.0);\n",
        "  return image.addBands(srcImg = opticalBands, overwrite = True)\\\n",
        "              .addBands(srcImg = thermalBands, overwrite = True)\n",
        "\n",
        "\n",
        "\n",
        "def download_data_ts_LANDSAT(R:RegionST, products, bands, path_save, scale=None, download_crop_size=500, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    times = (R.times[0], R.times[-1])\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size, cls=RegionST)\n",
        "    loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "    url_list.clear()\n",
        "    for j, R in loop:\n",
        "      region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" + f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "\n",
        "      # Merge products to single image collection\n",
        "      imCol = ee.ImageCollection(products[0])\n",
        "      imCol = filter_region(imCol, R, times=times, bands=bands)\n",
        "      imCol = ee.ImageCollection(imCol).map(applyScaleFactors_LANDSAT)\n",
        "      colList = imCol.toList(imCol.size())\n",
        "  \n",
        "      # Download each image\n",
        "      for i in range(colList.size().getInfo()):\n",
        "          image = ee.Image(colList.get(i))\n",
        "          url_list.append(str(image.getDownloadURL({'scale': scale, 'crs': 'EPSG:4326','region': f'{region}'})))\n",
        "\n",
        "\n",
        "    #download function in parallel fashion from collected urls\n",
        "    download_parallel(url_list)   \n",
        "\n",
        "    # Merge files\n",
        "    suffix = '.tif'\n",
        "    files = path_save.ls(include=[suffix])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 6])\n",
        "    ids = np.unique([int(o.split('_')[-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 6])\n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        if len(fs) < 500:\n",
        "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "            merge_tifs(fs, fsave, delete=True)\n",
        "        else:\n",
        "            fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
        "            if len(fs[(len(fs)//500)*500:]) > 0:\n",
        "                fs_break.append(fs[(len(fs)//500)*500:])\n",
        "            for fsi, fs2 in enumerate(fs_break):\n",
        "                fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
        "                merge_tifs(fs2, fsave, delete=True)\n",
        "\n",
        "    files = path_save.ls(include=[suffix, '_break'])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 11])\n",
        "    ids = np.unique([o.split('_')[-1]\n",
        "                     for o in files if len(o.split('_')[-1]) < 11])\n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "        merge_tifs(fs, fsave, delete=True)\n",
        "\n",
        "\n",
        "def download_data_LANDSAT(R:RegionST, times, products, bands, path_save, scale=None, max_cloud_fraction=None, use_least_cloudy=None, download_crop_size=500, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    if not ((path_save/f'download.{bands[0]}.tif').is_file() and \n",
        "           (path_save/f'download.{bands[1]}.tif').is_file() and\n",
        "           (path_save/f'download.{bands[2]}.tif').is_file()):\n",
        "        sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size, cls=RegionST)\n",
        "        fsaves = []\n",
        "        #for j, R in tqdm(enumerate(sR), total=len(sR)):\n",
        "        loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "        url_list.clear()\n",
        "        for j, R in loop:\n",
        "            region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" + f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "\n",
        "            if not ((path_save/f'download.{bands[0]}_{j}.tif').is_file() and \n",
        "                   (path_save/f'download.{bands[1]}_{j}.tif').is_file() and\n",
        "                   (path_save/f'download.{bands[2]}_{j}.tif').is_file()):\n",
        "                # Merge products to single image collection\n",
        "                imCol = ee.ImageCollection(products[0])\n",
        "                imCol = filter_region(imCol, R, times=times, bands=bands)\n",
        "                if max_cloud_fraction is not None:\n",
        "                    imCol = filter_cloudy(imCol, max_cloud_fraction=max_cloud_fraction)\n",
        "                if use_least_cloudy is not None:\n",
        "                    imCol = n_least_cloudy(imCol, n=use_least_cloudy)\n",
        "                im = imCol.median()\n",
        "                imCol = ee.ImageCollection([im])\n",
        "                colList = imCol.toList(imCol.size())\n",
        "                # info = colList.getInfo()\n",
        "                # data_times = [pd.to_datetime(o['properties']['system:time_start'], unit='ms') for o in info]\n",
        "                # data_cloudy = [o['properties']['CLOUDY_PIXEL_PERCENTAGE'] for o in info]\n",
        "                # Download each image\n",
        "                for i in range(colList.size().getInfo()):\n",
        "                    image = ee.Image(colList.get(i))\n",
        "                    fname = 'download'\n",
        "                    #fname = image.get('system:id').getInfo().split('/')[-1]\n",
        "                    fnames_full = [f'{fname}.{b}.tif' for b in bands]\n",
        "                    fnames_partial0 = [f'{fname}.{b}_{j}.tif' for b in bands]\n",
        "                    fnames_full = all([(path_save/f).is_file() for f in fnames_full])\n",
        "                    fnames_partial = all([(path_save/f).is_file() for f in fnames_partial0])\n",
        "                    if not fnames_full:\n",
        "                        fsaves.append([path_save/f for f in fnames_partial0])\n",
        "                        if not fnames_partial:\n",
        "                          url_list.append(str(image.getDownloadURL({'scale': scale, 'crs': 'EPSG:4326','region': f'{region}'})))\n",
        "\n",
        "        #download function in parallel fashion from collected urls\n",
        "        download_parallel(url_list) \n",
        "\n",
        "        # Merge files\n",
        "        suffix = '.tif'\n",
        "        files = path_save.ls(include=[suffix])\n",
        "        #files = np.unique(fsaves) \n",
        "        files = [o.stem for o in files]\n",
        "        ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 6])\n",
        "        ids = np.unique([int(o.split('_')[-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 6])\n",
        "        #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "        file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                    if f'{r}_{i}' in files] for r in ref] \n",
        "        for fs in file_groups:\n",
        "            if len(fs) < 500:\n",
        "                fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "                merge_tifs(fs, fsave, delete=True)\n",
        "            else:\n",
        "                fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
        "                if len(fs[(len(fs)//500)*500:]) > 0:\n",
        "                    fs_break.append(fs[(len(fs)//500)*500:])\n",
        "                for fsi, fs2 in enumerate(fs_break):\n",
        "                    fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
        "                    merge_tifs(fs2, fsave, delete=True)\n",
        "\n",
        "        files = path_save.ls(include=[suffix, '_break'])\n",
        "        files = [o.stem for o in files]\n",
        "        ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 11])\n",
        "        ids = np.unique([o.split('_')[-1]\n",
        "                         for o in files if len(o.split('_')[-1]) < 11])\n",
        "        #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "        file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                    if f'{r}_{i}' in files] for r in ref] \n",
        "        for fs in file_groups:\n",
        "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "            merge_tifs(fs, fsave, delete=True)"
      ],
      "metadata": {
        "id": "zPbn_-Fgjxin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from FireHR.data import *\n",
        "\n",
        "# Bounding box coordinates\n",
        "left   = 75.979728\n",
        "right  = 77.866667\n",
        "bottom = 31.453599\n",
        "top    = 32.416667\n",
        "\n",
        "path_save   = Path('data')\n",
        "# products    = [\"LANDSAT/LC09/C02/T2_L2\"]  # Product id in google earth engine\n",
        "products    = [\"LANDSAT/LC08/C02/T2_L2\"]  # Product id in google earth engine\n",
        "bands       = ['SR_B1', 'SR_B2','SR_B3','SR_B4','SR_B5','SR_B6','SR_B7','ST_B10'] # Red, Green, Blue\n",
        "\n",
        "\n",
        "R = RegionST(name         = 'TeslaGigaBerlin', \n",
        "             bbox         = [left,bottom,right,top], \n",
        "             scale_meters = 10, \n",
        "             time_start   = '2020-04-01', \n",
        "             time_end     = '2020-04-25')\n",
        "\n",
        "# Download time series\n",
        "# download_data_ts(R, products, bands, path_save)\n",
        "\n",
        "time_window = R.times[0], R.times[-1]\n",
        "\n",
        "# Download median composite of the 3 least cloudy images within the time_window\n",
        "# download_data_LANDSAT(R, time_window, products, bands, path_save, use_least_cloudy=3, show_progress=True)\n",
        "\n",
        "download_data_ts_LANDSAT(R, products, bands, path_save, show_progress=True)"
      ],
      "metadata": {
        "id": "AfcCwQBujYE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**COPERNICUS/S5P/OFFL/L3_CLOUD**"
      ],
      "metadata": {
        "id": "ks5FFYsgcK-G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**code**"
      ],
      "metadata": {
        "id": "5KdDjQLdcxQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "import ee\n",
        "import os\n",
        "import requests\n",
        "import rasterio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import json\n",
        "from IPython.core.debugger import set_trace\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "from fastprogress.fastprogress import progress_bar\n",
        "from banet.geo import open_tif, merge, Region\n",
        "from banet.geo import downsample\n",
        "   \n",
        "   \n",
        "import requests\n",
        "import time\n",
        "from multiprocessing import cpu_count\n",
        "from multiprocessing.pool import ThreadPool\n",
        "from google.colab import output\n",
        "from FireHR.data import *\n",
        "    \n",
        "cpus = cpu_count()\n",
        "url_list = []\n",
        "\n",
        "###########################################################################################################################\n",
        "def download_url(url):\n",
        "  r = requests.get(url)\n",
        "  with open(str(path_save/'data.zip'), 'wb') as f:\n",
        "      f.write(r.content)\n",
        "  with zipfile.ZipFile(str(path_save/'data.zip'), 'r') as f:\n",
        "      files = f.namelist()\n",
        "      f.extractall(str(path_save))\n",
        "  os.remove(str(path_save/'data.zip'))\n",
        "  print(\"downloading ğŸ”»:\",url_list.index(url),\"/\",len(url_list))\n",
        "  for f in files:\n",
        "      f = path_save/f\n",
        "      os.rename(str(f), str(path_save/f'{f.stem}_{url_list.index(url)}{f.suffix}'))\n",
        "###########################################################################################################################\n",
        "        \n",
        "\n",
        "def download_parallel(args):\n",
        "    cpus = cpu_count()\n",
        "    results = ThreadPool(cpus - 1).imap_unordered(download_url, args)\n",
        "    for result in results:\n",
        "        print('', result)\n",
        "        # output.clear()\n",
        "###########################################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "def download_data_ts_L3_CLOUD(R:RegionST, products, bands, path_save, scale=None, download_crop_size=1000, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    times = (R.times[0], R.times[-1])\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size, cls=RegionST)\n",
        "    loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "    url_list.clear()\n",
        "    for j, R in loop:\n",
        "      \n",
        "      region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" + f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "\n",
        "      # Merge products to single image collection\n",
        "      imCol = ee.ImageCollection(products[0])\n",
        "      imCol = filter_region(imCol, R, times=times, bands=bands)\n",
        "      imCol = ee.ImageCollection(imCol)\n",
        "      colList = imCol.toList(imCol.size())\n",
        "  \n",
        "      # Download each image\n",
        "      for i in range(colList.size().getInfo()):\n",
        "          print(\"collecting ğŸ”»:\",j)\n",
        "          image = ee.Image(colList.get(i))\n",
        "          url_list.append(str(image.getDownloadURL({'scale': scale, 'crs': 'EPSG:4326','region': f'{region}'})))\n",
        "          output.clear()\n",
        "\n",
        "\n",
        "    #download function in parallel fashion from collected urls\n",
        "    download_parallel(url_list)   \n",
        "\n",
        "    # Merge files\n",
        "    suffix = '.tif'\n",
        "    files = path_save.ls(include=[suffix])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 6])\n",
        "    ids = np.unique([int(o.split('_')[-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 6])\n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        if len(fs) < 500:\n",
        "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "            merge_tifs(fs, fsave, delete=True)\n",
        "        else:\n",
        "            fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
        "            if len(fs[(len(fs)//500)*500:]) > 0:\n",
        "                fs_break.append(fs[(len(fs)//500)*500:])\n",
        "            for fsi, fs2 in enumerate(fs_break):\n",
        "                fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
        "                merge_tifs(fs2, fsave, delete=True)\n",
        "\n",
        "    files = path_save.ls(include=[suffix, '_break'])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 11])\n",
        "    ids = np.unique([o.split('_')[-1]\n",
        "                     for o in files if len(o.split('_')[-1]) < 11])\n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "        merge_tifs(fs, fsave, delete=True)\n",
        "\n",
        "\n",
        "def download_data_L3_CLOUD(R:RegionST, times, products, bands, path_save, scale=None, max_cloud_fraction=None, use_least_cloudy=None, download_crop_size=1000, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    if not ((path_save/f'download.{bands[0]}.tif').is_file() and \n",
        "           (path_save/f'download.{bands[1]}.tif').is_file() and\n",
        "           (path_save/f'download.{bands[2]}.tif').is_file()):\n",
        "        sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size, cls=RegionST)\n",
        "        fsaves = []\n",
        "        #for j, R in tqdm(enumerate(sR), total=len(sR)):\n",
        "        loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "        url_list.clear()\n",
        "        for j, R in loop:\n",
        "            region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" + f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "\n",
        "            if not ((path_save/f'download.{bands[0]}_{j}.tif').is_file() and \n",
        "                   (path_save/f'download.{bands[1]}_{j}.tif').is_file() and\n",
        "                   (path_save/f'download.{bands[2]}_{j}.tif').is_file()):\n",
        "                # Merge products to single image collection\n",
        "                imCol = ee.ImageCollection(products[0])\n",
        "                imCol = filter_region(imCol, R, times=times, bands=bands)\n",
        "                if max_cloud_fraction is not None:\n",
        "                    imCol = filter_cloudy(imCol, max_cloud_fraction=max_cloud_fraction)\n",
        "                if use_least_cloudy is not None:\n",
        "                    imCol = n_least_cloudy(imCol, n=use_least_cloudy)\n",
        "                im = imCol.median()\n",
        "                imCol = ee.ImageCollection([im])\n",
        "                colList = imCol.toList(imCol.size())\n",
        "                # info = colList.getInfo()\n",
        "                # data_times = [pd.to_datetime(o['properties']['system:time_start'], unit='ms') for o in info]\n",
        "                # data_cloudy = [o['properties']['CLOUDY_PIXEL_PERCENTAGE'] for o in info]\n",
        "                # Download each image\n",
        "                for i in range(colList.size().getInfo()):\n",
        "                    image = ee.Image(colList.get(i))\n",
        "                    fname = 'download'\n",
        "                    #fname = image.get('system:id').getInfo().split('/')[-1]\n",
        "                    fnames_full = [f'{fname}.{b}.tif' for b in bands]\n",
        "                    fnames_partial0 = [f'{fname}.{b}_{j}.tif' for b in bands]\n",
        "                    fnames_full = all([(path_save/f).is_file() for f in fnames_full])\n",
        "                    fnames_partial = all([(path_save/f).is_file() for f in fnames_partial0])\n",
        "                    if not fnames_full:\n",
        "                        fsaves.append([path_save/f for f in fnames_partial0])\n",
        "                        if not fnames_partial:\n",
        "                          print(\"collecting ğŸ”»:\",j)\n",
        "                          url_list.append(str(image.getDownloadURL({'scale': scale, 'crs': 'EPSG:4326','region': f'{region}'})))\n",
        "                          output.clear()\n",
        "\n",
        "        #download function in parallel fashion from collected urls\n",
        "        download_parallel(url_list) \n",
        "\n",
        "        # Merge files\n",
        "        suffix = '.tif'\n",
        "        files = path_save.ls(include=[suffix])\n",
        "        #files = np.unique(fsaves) \n",
        "        files = [o.stem for o in files]\n",
        "        ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 6])\n",
        "        ids = np.unique([int(o.split('_')[-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 6])\n",
        "        #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "        file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                    if f'{r}_{i}' in files] for r in ref] \n",
        "        for fs in file_groups:\n",
        "            if len(fs) < 500:\n",
        "                fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "                merge_tifs(fs, fsave, delete=True)\n",
        "            else:\n",
        "                fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
        "                if len(fs[(len(fs)//500)*500:]) > 0:\n",
        "                    fs_break.append(fs[(len(fs)//500)*500:])\n",
        "                for fsi, fs2 in enumerate(fs_break):\n",
        "                    fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
        "                    merge_tifs(fs2, fsave, delete=True)\n",
        "\n",
        "        files = path_save.ls(include=[suffix, '_break'])\n",
        "        files = [o.stem for o in files]\n",
        "        ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 11])\n",
        "        ids = np.unique([o.split('_')[-1]\n",
        "                         for o in files if len(o.split('_')[-1]) < 11])\n",
        "        #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "        file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                    if f'{r}_{i}' in files] for r in ref] \n",
        "        for fs in file_groups:\n",
        "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "            merge_tifs(fs, fsave, delete=True)"
      ],
      "metadata": {
        "id": "Kb2D2Yvvc0-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/data"
      ],
      "metadata": {
        "id": "BWl8x-Ynk7VU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from FireHR.data import *\n",
        "# Bounding box coordinates\n",
        "left   = 75.979728\n",
        "right  = 77.866667\n",
        "bottom = 31.453599\n",
        "top    = 32.416667\n",
        "\n",
        "path_save   = Path('data')\n",
        "# products    = [\"LANDSAT/LC09/C02/T2_L2\"]  # Product id in google earth engine\n",
        "products    = [\"COPERNICUS/S5P/OFFL/L3_CLOUD\"]  # Product id in google earth engine\n",
        "bands       = ['cloud_top_height', 'cloud_base_height'] # Red, Green, Blue\n",
        "\n",
        "\n",
        "###Dataset Availability    2018-07-04T11:34:21Zâ€“2023-01-25T09:44:34\n",
        "R = RegionST(name         = 'TeslaGigaBerlin', \n",
        "             bbox         = [left,bottom,right,top], \n",
        "             scale_meters = 1113.2, \n",
        "             time_start   = '2021-05-10', \n",
        "             time_end     = '2021-06-20')\n",
        "\n",
        "# Download time series\n",
        "# download_data_ts(R, products, bands, path_save)\n",
        "\n",
        "time_window = R.times[0], R.times[-1]\n",
        "\n",
        "# Download median composite of the 3 least cloudy images within the time_window\n",
        "# download_data_L3_CLOUD(R, time_window, products, bands, path_save, use_least_cloudy=3, show_progress=True)\n",
        "\n",
        "download_data_ts_L3_CLOUD(R, products, bands, path_save, show_progress=True)"
      ],
      "metadata": {
        "id": "J1BplFXZcMuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**COPERNICUS/S1_GRD**"
      ],
      "metadata": {
        "id": "P7O0vJ9p-2H4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "import ee\n",
        "import os\n",
        "import requests\n",
        "import rasterio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import json\n",
        "from IPython.core.debugger import set_trace\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "from fastprogress.fastprogress import progress_bar\n",
        "from banet.geo import open_tif, merge, Region\n",
        "from banet.geo import downsample\n",
        "   \n",
        "   \n",
        "import requests\n",
        "import time\n",
        "from multiprocessing import cpu_count\n",
        "from multiprocessing.pool import ThreadPool\n",
        "from google.colab import output\n",
        "from FireHR.data import *\n",
        "   \n",
        "cpus = cpu_count()\n",
        "url_list = []\n",
        "\n",
        "###########################################################################################################################\n",
        "def download_url(url):\n",
        "  r = requests.get(url)\n",
        "  with open(str(path_save/'data.zip'), 'wb') as f:\n",
        "      f.write(r.content)\n",
        "  with zipfile.ZipFile(str(path_save/'data.zip'), 'r') as f:\n",
        "      files = f.namelist()\n",
        "      f.extractall(str(path_save))\n",
        "  os.remove(str(path_save/'data.zip'))\n",
        "  print(\"downloading ğŸ”»:\",url_list.index(url),\"/\",len(url_list))\n",
        "  for f in files:\n",
        "      f = path_save/f\n",
        "      os.rename(str(f), str(path_save/f'{f.stem}_{url_list.index(url)}{f.suffix}'))\n",
        "###########################################################################################################################\n",
        "        \n",
        "\n",
        "def download_parallel(args):\n",
        "    cpus = cpu_count()\n",
        "    results = ThreadPool(cpus - 1).imap_unordered(download_url, args)\n",
        "    for result in results:\n",
        "        print('', result)\n",
        "        output.clear()\n",
        "###########################################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def download_data_ts_S1_GRD(R:RegionST, products, bands, path_save, scale=None, download_crop_size=1000, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    times = (R.times[0], R.times[-1])\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size, cls=RegionST)\n",
        "    loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "    url_list.clear()\n",
        "    for j, R in loop:\n",
        "      region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" + f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "\n",
        "      # Merge products to single image collection\n",
        "      imCol = ee.ImageCollection(products[0])\n",
        "      imCol = filter_region(imCol, R, times=times, bands=bands)\n",
        "      imCol = ee.ImageCollection(imCol)\n",
        "      colList = imCol.toList(imCol.size())\n",
        "  \n",
        "      # Download each image\n",
        "      for i in range(colList.size().getInfo()):\n",
        "          image = ee.Image(colList.get(i))\n",
        "          url_list.append(str(image.getDownloadURL({'scale': scale, 'crs': 'EPSG:4326','region': f'{region}'})))\n",
        "\n",
        "\n",
        "    #download function in parallel fashion from collected urls\n",
        "    download_parallel(url_list)   \n",
        "\n",
        "    # Merge files\n",
        "    suffix = '.tif'\n",
        "    files = path_save.ls(include=[suffix])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 6])\n",
        "    ids = np.unique([int(o.split('_')[-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 6])\n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        if len(fs) < 500:\n",
        "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "            merge_tifs(fs, fsave, delete=True)\n",
        "        else:\n",
        "            fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
        "            if len(fs[(len(fs)//500)*500:]) > 0:\n",
        "                fs_break.append(fs[(len(fs)//500)*500:])\n",
        "            for fsi, fs2 in enumerate(fs_break):\n",
        "                fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
        "                merge_tifs(fs2, fsave, delete=True)\n",
        "\n",
        "    files = path_save.ls(include=[suffix, '_break'])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 11])\n",
        "    ids = np.unique([o.split('_')[-1]\n",
        "                     for o in files if len(o.split('_')[-1]) < 11])\n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "        merge_tifs(fs, fsave, delete=True)\n",
        "\n",
        "\n",
        "def download_data_S1_GRD(R:RegionST, times, products, bands, path_save, scale=None, max_cloud_fraction=None, use_least_cloudy=None, download_crop_size=1000, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size, cls=RegionST)\n",
        "    # print(sR)\n",
        "    fsaves = []\n",
        "    #for j, R in tqdm(enumerate(sR), total=len(sR)):\n",
        "    loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "    url_list.clear()\n",
        "    for j, R in loop:\n",
        "        print(R)\n",
        "        region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" + f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "\n",
        "        # Merge products to single image collection\n",
        "        imCol = ee.ImageCollection(products[0])\n",
        "        imCol = filter_region(imCol, R, times=times, bands=bands)\n",
        "        if max_cloud_fraction is not None:\n",
        "            imCol = filter_cloudy(imCol, max_cloud_fraction=max_cloud_fraction)\n",
        "        if use_least_cloudy is not None:\n",
        "            imCol = n_least_cloudy(imCol, n=use_least_cloudy)\n",
        "        im = imCol.median()\n",
        "        imCol = ee.ImageCollection([im])\n",
        "        colList = imCol.toList(imCol.size())\n",
        "        # info = colList.getInfo()\n",
        "        # data_times = [pd.to_datetime(o['properties']['system:time_start'], unit='ms') for o in info]\n",
        "        # data_cloudy = [o['properties']['CLOUDY_PIXEL_PERCENTAGE'] for o in info]\n",
        "        # Download each image\n",
        "        for i in range(colList.size().getInfo()):\n",
        "            image = ee.Image(colList.get(i))\n",
        "            fname = 'download'\n",
        "            #fname = image.get('system:id').getInfo().split('/')[-1]\n",
        "            fnames_full = [f'{fname}.{b}.tif' for b in bands]\n",
        "            fnames_partial0 = [f'{fname}.{b}_{j}.tif' for b in bands]\n",
        "            fnames_full = all([(path_save/f).is_file() for f in fnames_full])\n",
        "            fnames_partial = all([(path_save/f).is_file() for f in fnames_partial0])\n",
        "            if not fnames_full:\n",
        "                fsaves.append([path_save/f for f in fnames_partial0])\n",
        "                if not fnames_partial:\n",
        "                  url_list.append(str(image.getDownloadURL({'scale': scale, 'crs': 'EPSG:4326','region': f'{region}'})))\n",
        "\n",
        "    #download function in parallel fashion from collected urls\n",
        "    download_parallel(url_list) \n",
        "\n",
        "    # Merge files\n",
        "    suffix = '.tif'\n",
        "    files = path_save.ls(include=[suffix])\n",
        "    #files = np.unique(fsaves) \n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                      for o in files if len(o.split('_')[-1]) < 6])\n",
        "    ids = np.unique([int(o.split('_')[-1]) \n",
        "                      for o in files if len(o.split('_')[-1]) < 6])\n",
        "    #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        if len(fs) < 500:\n",
        "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "            merge_tifs(fs, fsave, delete=True)\n",
        "        else:\n",
        "            fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
        "            if len(fs[(len(fs)//500)*500:]) > 0:\n",
        "                fs_break.append(fs[(len(fs)//500)*500:])\n",
        "            for fsi, fs2 in enumerate(fs_break):\n",
        "                fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
        "                merge_tifs(fs2, fsave, delete=True)\n",
        "\n",
        "    files = path_save.ls(include=[suffix, '_break'])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                      for o in files if len(o.split('_')[-1]) < 11])\n",
        "    ids = np.unique([o.split('_')[-1]\n",
        "                      for o in files if len(o.split('_')[-1]) < 11])\n",
        "    #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "        merge_tifs(fs, fsave, delete=True)\n"
      ],
      "metadata": {
        "id": "rafg5yLa-5FB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "import ee\n",
        "import os\n",
        "import requests\n",
        "import rasterio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import json\n",
        "from IPython.core.debugger import set_trace\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "from fastprogress.fastprogress import progress_bar\n",
        "from banet.geo import open_tif, merge, Region\n",
        "from banet.geo import downsample\n",
        "   \n",
        "   \n",
        "import requests\n",
        "import time\n",
        "from multiprocessing import cpu_count\n",
        "from multiprocessing.pool import ThreadPool\n",
        "from google.colab import output\n",
        "from FireHR.data import *\n",
        "   \n",
        "cpus = cpu_count()\n",
        "url_list = []\n",
        "\n",
        "###########################################################################################################################\n",
        "def download_url(xxx):\n",
        "  print(\"DownloadingğŸ”»\")\n",
        "  j = (i for i, item in enumerate(uni) if item == xxx)\n",
        "  R = xxx\n",
        "  region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" + f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "  # Merge products to single image collection\n",
        "  imCol = ee.ImageCollection(products[0])\n",
        "  imCol = filter_region(imCol, R, times=time_window, bands=bands)\n",
        "  im = imCol.median()\n",
        "  imCol = ee.ImageCollection([im])\n",
        "  colList = imCol.toList(imCol.size())\n",
        "  # info = colList.getInfo()\n",
        "  # data_times = [pd.to_datetime(o['properties']['system:time_start'], unit='ms') for o in info]\n",
        "  # data_cloudy = [o['properties']['CLOUDY_PIXEL_PERCENTAGE'] for o in info]\n",
        "  # Download each image\n",
        "  for i in range(colList.size().getInfo()):\n",
        "      image = ee.Image(colList.get(i))\n",
        "      fname = 'download'\n",
        "      #fname = image.get('system:id').getInfo().split('/')[-1]\n",
        "      fnames_full = [f'{fname}.{b}.tif' for b in bands]\n",
        "      fnames_partial0 = [f'{fname}.{b}_{j}.tif' for b in bands]\n",
        "      fnames_full = all([(path_save/f).is_file() for f in fnames_full])\n",
        "      fnames_partial = all([(path_save/f).is_file() for f in fnames_partial0])\n",
        "      if not fnames_full:\n",
        "          fsaves.append([path_save/f for f in fnames_partial0])\n",
        "          if not fnames_partial:\n",
        "              zip_error = True\n",
        "              for i in range(10): # Try 10 times\n",
        "                  if zip_error:\n",
        "                      try:\n",
        "                          url = image.getDownloadURL({'scale': scale, 'crs': 'EPSG:4326', 'region': f'{region}'})\n",
        "                          print(url)\n",
        "                          r = requests.get(url)\n",
        "                          with open(str(path_save/'data.zip'), 'wb') as f:\n",
        "                              f.write(r.content)\n",
        "                          with zipfile.ZipFile(str(path_save/'data.zip'), 'r') as f:\n",
        "                              files = f.namelist()\n",
        "                              f.extractall(str(path_save))\n",
        "                          os.remove(str(path_save/'data.zip'))\n",
        "                          zip_error = False\n",
        "                      except:\n",
        "                          zip_error = True\n",
        "                          os.remove(str(path_save/'data.zip'))\n",
        "                          time.sleep(2)\n",
        "              if zip_error: raise Exception(f'Failed to process {url}')\n",
        "              for f in files:\n",
        "                  f = path_save/f\n",
        "                  os.rename(str(f), str(path_save/f'{f.stem}_{j}{f.suffix}'))\n",
        "\n",
        "###########################################################################################################################\n",
        "        \n",
        "\n",
        "def download_parallel(args):\n",
        "    cpus = cpu_count()\n",
        "    results = ThreadPool(cpus - 1).imap_unordered(download_url, args)\n",
        "    for result in results:\n",
        "        print('', result)\n",
        "        # output.clear()\n",
        "###########################################################################################################################\n",
        "\n",
        "\n",
        "uni = []\n",
        "fsaves = []\n",
        "def download_data_S1_GRD(R:RegionST, times, products, bands, path_save, scale=None, download_crop_size=1000, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size, cls=RegionST)\n",
        "    # print(sR)\n",
        "    \n",
        "    uni.clear()\n",
        "    uni.append(sR)\n",
        "    \n",
        "    download_parallel(sR) \n",
        "\n",
        "        \n",
        "\n",
        "    #download function in parallel fashion from collected urls\n",
        "\n",
        "    # Merge files\n",
        "    suffix = '.tif'\n",
        "    files = path_save.ls(include=[suffix])\n",
        "    #files = np.unique(fsaves) \n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                      for o in files if len(o.split('_')[-1]) < 6])\n",
        "    ids = np.unique([int(o.split('_')[-1]) \n",
        "                      for o in files if len(o.split('_')[-1]) < 6])\n",
        "    #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        if len(fs) < 500:\n",
        "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "            merge_tifs(fs, fsave, delete=True)\n",
        "        else:\n",
        "            fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
        "            if len(fs[(len(fs)//500)*500:]) > 0:\n",
        "                fs_break.append(fs[(len(fs)//500)*500:])\n",
        "            for fsi, fs2 in enumerate(fs_break):\n",
        "                fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
        "                merge_tifs(fs2, fsave, delete=True)\n",
        "\n",
        "    files = path_save.ls(include=[suffix, '_break'])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                      for o in files if len(o.split('_')[-1]) < 11])\n",
        "    ids = np.unique([o.split('_')[-1]\n",
        "                      for o in files if len(o.split('_')[-1]) < 11])\n",
        "    #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "        merge_tifs(fs, fsave, delete=True)\n"
      ],
      "metadata": {
        "id": "fR_g_Ra9NkAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/data"
      ],
      "metadata": {
        "id": "Vb8Gdmr5_3xT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pathlib import Path\n",
        "from FireHR.data import *\n",
        "\n",
        "# Bounding box coordinates\n",
        "left   = 75.979728\n",
        "right  = 77.866667\n",
        "bottom = 31.453599\n",
        "top    = 32.416667\n",
        "\n",
        "\n",
        "path_save   = Path('data')\n",
        "products    = [\"JAXA/ALOS/PALSAR-2/Level2_2/ScanSAR\"]  # Product id in google earth engine\n",
        "bands       = ['HH', 'HV'] # Red, Green, Blue\n",
        "scale       = 10\n",
        "\n",
        "\n",
        "R = RegionST(name         = 'TeslaGigaBerlin', \n",
        "             bbox         = [left,bottom,right,top], \n",
        "             scale_meters = scale, \n",
        "             time_start   = '2020-02-01', \n",
        "             time_end     = '2020-04-25')\n",
        "\n",
        "# Download time series\n",
        "# download_data_ts(R, products, bands, path_save)\n",
        "\n",
        "time_window = R.times[0], R.times[-1]\n",
        "\n",
        "# Download median composite of the 3 least cloudy images within the time_window\n",
        "download_data_S1_GRD(R, time_window, products, bands, path_save, show_progress=True)\n",
        "\n",
        "# download_data_ts_S1_GRD(R, products, bands, path_save, show_progress=True)"
      ],
      "metadata": {
        "id": "dlBRU5LLABBv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}